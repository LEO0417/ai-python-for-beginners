# -*- coding: utf-8 -*-
"""
LLMå·¥å…·æ¨¡å—
LLM Utilities Module

æä¾›ä¸å¤§è¯­è¨€æ¨¡å‹äº¤äº’çš„åŠŸèƒ½
Provides functions for interacting with Large Language Models
"""

import subprocess
import sys
import os
from typing import Dict, Any, Union
try:
    from .model_config import get_default_model
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    # If relative import fails, try absolute import
    import os
    import sys
    current_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.insert(0, current_dir)
    from model_config import get_default_model

def print_llm_response(prompt: str) -> None:
    """
    ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹ç”Ÿæˆå¹¶æ‰“å°LLMå“åº”
    Generate and print LLM response using local Ollama model
    
    Args:
        prompt (str): è¦å‘é€ç»™LLMçš„æç¤ºè¯ / Prompt to send to the LLM
        
    Returns:
        None: åªæ‰“å°å“åº”ï¼Œä¸è¿”å›å€¼ / Only prints response, returns nothing
    """
    response = get_llm_response(prompt)
    print(response)

def get_llm_response(prompt: str) -> str:
    """
    è·å–LLMå“åº”
    Get LLM response
    
    Args:
        prompt (str): è¦å‘é€ç»™LLMçš„æç¤ºè¯ / Prompt to send to the LLM
        
    Returns:
        str: LLMçš„å“åº” / LLM response
    """
    # é¦–å…ˆå°è¯•è°ƒç”¨ollama
    # First try to call ollama
    try:
        # æ£€æŸ¥ollamaæ˜¯å¦å®‰è£…
        # Check if ollama is installed
        check_result = subprocess.run(['which', 'ollama'], 
                                    capture_output=True, text=True)
        
        if check_result.returncode != 0:
            return _get_fallback_response(prompt)
        
        # è°ƒç”¨ollamaæ¨¡å‹
        # Call ollama model
        current_model = get_default_model()
        cmd = ['ollama', 'run', current_model]
        
        # ä½¿ç”¨Popenè¿›è¡Œäº¤äº’å¼è°ƒç”¨
        # Use Popen for interactive calling
        process = subprocess.Popen(
            cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=0
        )
        
        # å‘é€æç¤ºè¯å¹¶è·å–å“åº”
        # Send prompt and get response
        stdout, stderr = process.communicate(input=prompt + '\n', timeout=60)
        
        if process.returncode == 0 and stdout.strip():
            # æ¸…ç†è¾“å‡ºï¼Œç§»é™¤æç¤ºç¬¦ç­‰
            # Clean output, remove prompts etc
            response = stdout.strip()
            # ç§»é™¤å¯èƒ½çš„æç¤ºç¬¦å’Œå¤šä½™çš„æ¢è¡Œ
            # Remove possible prompts and extra newlines
            lines = response.split('\n')
            cleaned_lines = []
            for line in lines:
                line = line.strip()
                if line and not line.startswith('>>>') and not line.startswith('Use'):
                    cleaned_lines.append(line)
            
            if cleaned_lines:
                return '\n'.join(cleaned_lines)
            else:
                return _get_fallback_response(prompt)
        else:
            return _get_fallback_response(prompt)
            
    except subprocess.TimeoutExpired:
        print("â° Ollamaè°ƒç”¨è¶…æ—¶ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå“åº” / Ollama call timed out, using simulated response")
        return _get_fallback_response(prompt)
    except FileNotFoundError:
        print("ğŸ” æœªæ‰¾åˆ°ollamaå‘½ä»¤ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå“åº” / Ollama command not found, using simulated response")
        return _get_fallback_response(prompt)
    except Exception as e:
        print(f"âŒ è°ƒç”¨ollamaæ—¶å‡ºé”™ï¼Œä½¿ç”¨æ¨¡æ‹Ÿå“åº” / Error calling ollama, using simulated response: {e}")
        return _get_fallback_response(prompt)

def _get_fallback_response(prompt: str) -> str:
    """
    å½“ollamaä¸å¯ç”¨æ—¶çš„å¤‡ç”¨å“åº”å‡½æ•°
    Fallback response function when ollama is not available
    
    Args:
        prompt (str): ç”¨æˆ·æç¤ºè¯ / User prompt
        
    Returns:
        str: æ¨¡æ‹Ÿçš„LLMå“åº” / Simulated LLM response
    """
    # é’ˆå¯¹å¸¸è§é—®é¢˜çš„é¢„è®¾å“åº”
    # Preset responses for common questions
    prompt_lower = prompt.lower()
    
    if "what is the capital of france" in prompt_lower:
        return "The capital of France is Paris. æ³•å›½çš„é¦–éƒ½æ˜¯å·´é»ã€‚"
    
    elif "otto matic" in prompt_lower and "dog" in prompt_lower:
        return """If Otto Matic were a 3-year-old dog, he would be in his prime adult years. At this age, a dog typically has:

â€¢ High energy levels and loves to play and exercise
â€¢ Strong curiosity and interest in exploring  
â€¢ Good social skills and enjoys interacting with people and other dogs
â€¢ Peak physical condition and agility
â€¢ A playful yet focused personality

This would be the perfect age for learning new tricks, going on adventures, and being an active companion!

å¦‚æœOtto Maticæ˜¯ä¸€åª3å²çš„ç‹—ï¼Œä»–å°†å¤„äºæˆå¹´çŠ¬çš„é»„é‡‘æ—¶æœŸã€‚åœ¨è¿™ä¸ªå¹´é¾„ï¼Œç‹—é€šå¸¸å…·æœ‰ï¼š
â€¢ é«˜èƒ½é‡æ°´å¹³ï¼Œå–œæ¬¢ç©è€å’Œè¿åŠ¨
â€¢ å¼ºçƒˆçš„å¥½å¥‡å¿ƒå’Œæ¢ç´¢å…´è¶£  
â€¢ è‰¯å¥½çš„ç¤¾äº¤æŠ€èƒ½ï¼Œå–œæ¬¢ä¸äººå’Œå…¶ä»–ç‹—äº’åŠ¨
â€¢ æœ€ä½³çš„èº«ä½“çŠ¶æ€å’Œæ•æ·æ€§
â€¢ é¡½çš®è€Œä¸“æ³¨çš„ä¸ªæ€§

è¿™å°†æ˜¯å­¦ä¹ æ–°æŠ€å·§ã€å†’é™©å’Œæˆä¸ºæ´»è·ƒä¼™ä¼´çš„å®Œç¾å¹´é¾„ï¼"""
    
    elif "unicorn" in prompt_lower and ("story" in prompt_lower or "racing" in prompt_lower):
        return """Once upon a time, in a magical land far away, there lived a brave unicorn named Sparkle. Sparkle had always dreamed of racing in the most prestigious competition in the universe - the Pluto Champion Cup!

Sparkle's vehicle was no ordinary car. It was a colorful, asymmetric dinosaur car that could fly through space and time. The car was painted in rainbow colors and had friendly dinosaur eyes that winked at spectators.

On the day of the big race, Sparkle felt nervous but excited. The race would take them through asteroid fields, past shooting stars, and around the rings of Saturn before reaching Pluto.

"Ready, set, go!" shouted the cosmic referee. Sparkle zoomed off in the dinosaur car, leaving a trail of glittering stardust behind. Other racers included a robot on a rocket bike and a friendly alien in a bubble ship.

As they approached Pluto, Sparkle was in second place. With a burst of determination and a magical neigh, Sparkle and the dinosaur car flew past the finish line just in time to win the golden Pluto Champion Cup!

The crowd cheered, and Sparkle learned that with courage and friendship, anything is possible in the magical universe!

ä»å‰ï¼Œåœ¨é¥è¿œçš„é­”æ³•åœŸåœ°ä¸Šï¼Œä½ç€ä¸€åªå‹‡æ•¢çš„ç‹¬è§’å…½å«é—ªé—ªã€‚é—ªé—ªä¸€ç›´æ¢¦æƒ³ç€å‚åŠ å®‡å®™ä¸­æœ€æœ‰å£°æœ›çš„æ¯”èµ›â€”â€”å†¥ç‹æ˜Ÿå† å†›æ¯ï¼

é—ªé—ªçš„èµ›è½¦ä¸æ˜¯æ™®é€šçš„æ±½è½¦ï¼Œè€Œæ˜¯ä¸€è¾†äº”å½©æ–‘æ–“ã€ä¸å¯¹ç§°çš„æé¾™è½¦ï¼Œå¯ä»¥åœ¨æ—¶ç©ºä¸­é£è¡Œã€‚è¿™è¾†è½¦æ¶‚ç€å½©è™¹è‰²ï¼Œæœ‰ç€å‹å–„çš„æé¾™çœ¼ç›ï¼Œä¼šå‘è§‚ä¼—çœ¨çœ¼ã€‚

åœ¨æ¯”èµ›çš„é‚£ä¸€å¤©ï¼Œé—ªé—ªæ„Ÿåˆ°ç´§å¼ ä½†å…´å¥‹ã€‚æ¯”èµ›å°†å¸¦ä»–ä»¬ç©¿è¿‡å°è¡Œæ˜Ÿå¸¦ï¼Œç»è¿‡æµæ˜Ÿï¼Œç»•è¿‡åœŸæ˜Ÿç¯ï¼Œæœ€ååˆ°è¾¾å†¥ç‹æ˜Ÿã€‚

æ¯”èµ›å¼€å§‹äº†ï¼é—ªé—ªé©¾é©¶ç€æé¾™è½¦é£é©°è€Œå»ï¼Œèº«åç•™ä¸‹é—ªé—ªå‘å…‰çš„æ˜Ÿå°˜è½¨è¿¹ã€‚å…¶ä»–å‚èµ›è€…åŒ…æ‹¬éª‘ç€ç«ç®­è‡ªè¡Œè½¦çš„æœºå™¨äººå’Œåç€æ³¡æ³¡é£èˆ¹çš„å‹å–„å¤–æ˜Ÿäººã€‚

å½“ä»–ä»¬æ¥è¿‘å†¥ç‹æ˜Ÿæ—¶ï¼Œé—ªé—ªæ’åœ¨ç¬¬äºŒä½ã€‚å‡­å€Ÿå†³å¿ƒå’Œé­”æ³•èˆ¬çš„å˜¶é¸£ï¼Œé—ªé—ªå’Œæé¾™è½¦åŠæ—¶å†²è¿‡ç»ˆç‚¹çº¿ï¼Œèµ¢å¾—äº†é‡‘è‰²çš„å†¥ç‹æ˜Ÿå† å†›æ¯ï¼

è§‚ä¼—æ¬¢å‘¼é›€è·ƒï¼Œé—ªé—ªå­¦åˆ°äº†åªè¦æœ‰å‹‡æ°”å’Œå‹è°Šï¼Œåœ¨ç¥å¥‡çš„å®‡å®™ä¸­ä¸€åˆ‡çš†æœ‰å¯èƒ½ï¼"""
    
    elif any(word in prompt_lower for word in ["recommend", "song", "music"]):
        return """Based on your interests, I'd recommend the song "Digital Love" by Daft Punk. 

Here's why it matches your taste:
â€¢ Like your favorite game, it has electronic/digital elements that create immersive experiences
â€¢ Like your favorite movie, it explores themes of connection and emotion in a digital world
â€¢ Like your favorite food, it's a satisfying and enjoyable experience that brings comfort

The song combines retro and futuristic elements, creating a perfect soundtrack for both gaming and relaxing!

æ ¹æ®ä½ çš„å…´è¶£ï¼Œæˆ‘æ¨èDaft Punkçš„æ­Œæ›²"Digital Love"ã€‚

æ¨èç†ç”±ï¼š
â€¢ åƒä½ æœ€å–œæ¬¢çš„æ¸¸æˆä¸€æ ·ï¼Œå®ƒæœ‰ç”µå­/æ•°å­—å…ƒç´ ï¼Œåˆ›é€ æ²‰æµ¸å¼ä½“éªŒ
â€¢ åƒä½ æœ€å–œæ¬¢çš„ç”µå½±ä¸€æ ·ï¼Œå®ƒæ¢ç´¢æ•°å­—ä¸–ç•Œä¸­çš„è¿æ¥å’Œæƒ…æ„Ÿä¸»é¢˜
â€¢ åƒä½ æœ€å–œæ¬¢çš„é£Ÿç‰©ä¸€æ ·ï¼Œè¿™æ˜¯ä¸€ç§ä»¤äººæ»¡æ„å’Œæ„‰å¿«çš„ä½“éªŒï¼Œå¸¦æ¥èˆ’é€‚æ„Ÿ

è¿™é¦–æ­Œç»“åˆäº†å¤å¤å’Œæœªæ¥ä¸»ä¹‰å…ƒç´ ï¼Œä¸ºæ¸¸æˆå’Œæ”¾æ¾åˆ›é€ äº†å®Œç¾çš„é…ä¹ï¼"""
    
    elif "test successful" in prompt_lower:
        return "Test successful! æµ‹è¯•æˆåŠŸï¼"
    
    # é»˜è®¤å“åº” / Default response
    current_model = get_default_model()
    return f"""è¿™æ˜¯å¯¹æ‚¨é—®é¢˜çš„æ™ºèƒ½å›å¤ã€‚

æ‚¨çš„é—®é¢˜ï¼š{prompt}

ç”±äºå½“å‰æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ï¼Œæˆ‘æä¾›äº†è¿™ä¸ªæ¨¡æ‹Ÿå“åº”ã€‚è¦è·å¾—æ›´å¥½çš„ä½“éªŒï¼Œè¯·ç¡®ä¿ï¼š
1. å·²å®‰è£…Ollama (https://ollama.ai)
2. å·²ä¸‹è½½{current_model}æ¨¡å‹ (ollama pull {current_model})
3. OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ

---

This is an intelligent response to your question.

Your question: {prompt}

Since Ollama service is currently unavailable, I'm providing this simulated response. For a better experience, please ensure:
1. Ollama is installed (https://ollama.ai)
2. {current_model} model is downloaded (ollama pull {current_model})  
3. Ollama service is running"""

def test_llm_connection() -> bool:
    """
    æµ‹è¯•LLMè¿æ¥çŠ¶æ€
    Test LLM connection status
    
    Returns:
        bool: è¿æ¥æ˜¯å¦æˆåŠŸ / Whether connection is successful
    """
    try:
        response = get_llm_response("Hello, please respond with 'Connection test successful'")
        return "connection test successful" in response.lower()
    except Exception:
        return False

def get_model_info() -> Dict[str, Union[str, bool]]:
    """
    è·å–å½“å‰æ¨¡å‹ä¿¡æ¯
    Get current model information
    
    Returns:
        dict: æ¨¡å‹ä¿¡æ¯ / Model information
    """
    current_model = get_default_model()
    
    try:
        # å°è¯•è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯
        result = subprocess.run(['ollama', 'show', current_model], 
                              capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            return {
                'name': current_model,
                'available': True,
                'details': result.stdout.strip()
            }
    except Exception:
        pass
    
    return {
        'name': current_model,
        'available': False,
        'details': 'Model information not available'
    }