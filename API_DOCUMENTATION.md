# API Documentation / APIæ–‡æ¡£

## æ¦‚è¿° / Overview

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†AI Pythonåˆå­¦è€…æ•™ç¨‹é¡¹ç›®ä¸­`helper_functions`åŒ…çš„æ‰€æœ‰APIæ¥å£ã€‚

This document provides detailed information about all API interfaces in the `helper_functions` package of the AI Python for Beginners tutorial project.

## ç›®å½• / Table of Contents

- [æ¨¡å‹é…ç½®ç®¡ç† / Model Configuration Management](#æ¨¡å‹é…ç½®ç®¡ç†--model-configuration-management)
- [LLMå·¥å…·å‡½æ•° / LLM Utilities](#llmå·¥å…·å‡½æ•°--llm-utilities)
- [é€šç”¨å·¥å…·å‡½æ•° / Common Utilities](#é€šç”¨å·¥å…·å‡½æ•°--common-utilities)
- [ä½¿ç”¨ç¤ºä¾‹ / Usage Examples](#ä½¿ç”¨ç¤ºä¾‹--usage-examples)
- [é”™è¯¯å¤„ç† / Error Handling](#é”™è¯¯å¤„ç†--error-handling)

## æ¨¡å‹é…ç½®ç®¡ç† / Model Configuration Management

### `get_default_model()`

è·å–å½“å‰é»˜è®¤æ¨¡å‹åç§°ã€‚

Get the current default model name.

**è¿”å›å€¼ / Returns:**
- `str`: æ¨¡å‹åç§° / Model name

**ä¼˜å…ˆçº§é¡ºåº / Priority Order:**
1. ç¯å¢ƒå˜é‡ `OLLAMA_MODEL` / Environment variable `OLLAMA_MODEL`
2. è¯¾ç¨‹çº§é…ç½®æ–‡ä»¶ / Lesson-level configuration file
3. å…¨å±€é…ç½®æ–‡ä»¶ / Global configuration file
4. è‡ªåŠ¨æ£€æµ‹çš„ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹ / Auto-detected first available model
5. é»˜è®¤å›é€€å€¼ `"gemma3n:latest"` / Default fallback value `"gemma3n:latest"`

**ç¤ºä¾‹ / Example:**

```python
from helper_functions import get_default_model

# è·å–å½“å‰é»˜è®¤æ¨¡å‹ / Get current default model
current_model = get_default_model()
print(f"å½“å‰æ¨¡å‹ / Current model: {current_model}")
```

### `set_default_model(model_name, scope='global')`

è®¾ç½®é»˜è®¤æ¨¡å‹åˆ°é…ç½®æ–‡ä»¶ã€‚

Set default model to configuration file.

**å‚æ•° / Parameters:**
- `model_name` (str): æ¨¡å‹åç§° / Model name
- `scope` (str, optional): é…ç½®èŒƒå›´ï¼Œå¯é€‰å€¼ï¼š`'global'` æˆ– `'lesson'` / Configuration scope, options: `'global'` or `'lesson'`

**è¿”å›å€¼ / Returns:**
- `None`

**å¼‚å¸¸ / Exceptions:**
- å¦‚æœä¿å­˜é…ç½®å¤±è´¥ï¼Œä¼šæ‰“å°é”™è¯¯ä¿¡æ¯ / Prints error message if saving configuration fails

**ç¤ºä¾‹ / Example:**

```python
from helper_functions import set_default_model

# è®¾ç½®å…¨å±€é»˜è®¤æ¨¡å‹ / Set global default model
set_default_model("qwen3:0.6b", scope="global")

# è®¾ç½®è¯¾ç¨‹çº§é»˜è®¤æ¨¡å‹ / Set lesson-level default model
set_default_model("gemma3n:latest", scope="lesson")
```

### `get_available_models()`

è·å–æ‰€æœ‰å¯ç”¨çš„ollamaæ¨¡å‹åˆ—è¡¨ã€‚

Get list of all available ollama models.

**è¿”å›å€¼ / Returns:**
- `list[str]`: å¯ç”¨æ¨¡å‹åç§°åˆ—è¡¨ / List of available model names

**ç¤ºä¾‹ / Example:**

```python
from helper_functions import get_available_models

# è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ / Get available models list
models = get_available_models()
print("å¯ç”¨æ¨¡å‹ / Available models:")
for model in models:
    print(f"  - {model}")
```

### `show_model_info()`

æ˜¾ç¤ºå½“å‰æ¨¡å‹é…ç½®çš„è¯¦ç»†ä¿¡æ¯ã€‚

Show detailed information about current model configuration.

**è¿”å›å€¼ / Returns:**
- `None`

**è¾“å‡ºä¿¡æ¯åŒ…æ‹¬ / Output includes:**
- å½“å‰ä½¿ç”¨çš„æ¨¡å‹ / Currently used model
- ç¯å¢ƒå˜é‡è®¾ç½® / Environment variable settings
- è¯¾ç¨‹çº§é…ç½® / Lesson-level configuration
- å…¨å±€é…ç½® / Global configuration
- å¯ç”¨æ¨¡å‹åˆ—è¡¨ / Available models list

**ç¤ºä¾‹ / Example:**

```python
from helper_functions import show_model_info

# æ˜¾ç¤ºæ¨¡å‹é…ç½®ä¿¡æ¯ / Show model configuration info
show_model_info()
```

## LLMå·¥å…·å‡½æ•° / LLM Utilities

### `print_llm_response(prompt)`

ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹ç”Ÿæˆå¹¶æ‰“å°LLMå“åº”ã€‚

Generate and print LLM response using local Ollama model.

**å‚æ•° / Parameters:**
- `prompt` (str): è¦å‘é€ç»™LLMçš„æç¤ºè¯ / Prompt to send to the LLM

**è¿”å›å€¼ / Returns:**
- `None`: åªæ‰“å°å“åº”ï¼Œä¸è¿”å›å€¼ / Only prints response, returns nothing

**ç¤ºä¾‹ / Example:**

```python
from helper_functions import print_llm_response

# ç®€å•å¯¹è¯ / Simple conversation
print_llm_response("What is Python?")

# ä½¿ç”¨å˜é‡çš„åŠ¨æ€æç¤º / Dynamic prompt with variables
name = "Alice"
age = 25
print_llm_response(f"Tell me about a {age}-year-old person named {name}.")
```

### `get_llm_response(prompt)`

è·å–LLMå“åº”ä½†ä¸æ‰“å°ã€‚

Get LLM response without printing.

**å‚æ•° / Parameters:**
- `prompt` (str): è¦å‘é€ç»™LLMçš„æç¤ºè¯ / Prompt to send to the LLM

**è¿”å›å€¼ / Returns:**
- `str`: LLMçš„å“åº”æ–‡æœ¬ / LLM response text

**å¼‚å¸¸ / Exceptions:**
- å¦‚æœè°ƒç”¨å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯ / Returns error message if call fails

**ç¤ºä¾‹ / Example:**

```python
from helper_functions import get_llm_response

# è·å–å“åº”å¹¶å­˜å‚¨ / Get response and store
response = get_llm_response("Explain machine learning in simple terms.")
print(f"Response length: {len(response)} characters")

# å¤„ç†å“åº” / Process response
if "machine learning" in response.lower():
    print("Response contains the requested topic.")
```

### `test_llm_connection()`

æµ‹è¯•ä¸LLMçš„è¿æ¥çŠ¶æ€ã€‚

Test connection status with LLM.

**è¿”å›å€¼ / Returns:**
- `bool`: è¿æ¥æˆåŠŸè¿”å› `True`ï¼Œå¤±è´¥è¿”å› `False` / Returns `True` if connection successful, `False` otherwise

**ç¤ºä¾‹ / Example:**

```python
from helper_functions import test_llm_connection

# æµ‹è¯•è¿æ¥ / Test connection
if test_llm_connection():
    print("âœ… LLMè¿æ¥æ­£å¸¸ / LLM connection is working")
else:
    print("âŒ LLMè¿æ¥å¤±è´¥ / LLM connection failed")
```

## é€šç”¨å·¥å…·å‡½æ•° / Common Utilities

### `get_project_root()`

è·å–é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ã€‚

Get project root directory path.

**è¿”å›å€¼ / Returns:**
- `str`: é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„ / Absolute path to project root directory

**ç¤ºä¾‹ / Example:**

```python
from helper_functions import get_project_root

# è·å–é¡¹ç›®æ ¹ç›®å½• / Get project root directory
root_path = get_project_root()
print(f"é¡¹ç›®æ ¹ç›®å½• / Project root: {root_path}")
```

### `setup_logging(level='INFO')`

è®¾ç½®æ—¥å¿—è®°å½•é…ç½®ã€‚

Setup logging configuration.

**å‚æ•° / Parameters:**
- `level` (str, optional): æ—¥å¿—çº§åˆ«ï¼Œå¯é€‰å€¼ï¼š`'DEBUG'`, `'INFO'`, `'WARNING'`, `'ERROR'` / Log level, options: `'DEBUG'`, `'INFO'`, `'WARNING'`, `'ERROR'`

**è¿”å›å€¼ / Returns:**
- `None`

**ç¤ºä¾‹ / Example:**

```python
from helper_functions import setup_logging

# è®¾ç½®è°ƒè¯•çº§åˆ«æ—¥å¿— / Set debug level logging
setup_logging(level='DEBUG')

# è®¾ç½®ä¿¡æ¯çº§åˆ«æ—¥å¿— / Set info level logging
setup_logging(level='INFO')
```

## ä½¿ç”¨ç¤ºä¾‹ / Usage Examples

### åŸºç¡€ä½¿ç”¨ / Basic Usage

```python
# å¯¼å…¥æ‰€éœ€å‡½æ•° / Import required functions
from helper_functions import (
    print_llm_response,
    get_default_model,
    set_default_model,
    show_model_info
)

# 1. æ£€æŸ¥å½“å‰æ¨¡å‹ / Check current model
print(f"å½“å‰æ¨¡å‹ / Current model: {get_default_model()}")

# 2. æ˜¾ç¤ºè¯¦ç»†é…ç½®ä¿¡æ¯ / Show detailed configuration info
show_model_info()

# 3. ç®€å•çš„AIå¯¹è¯ / Simple AI conversation
print_llm_response("Hello! Please introduce yourself.")

# 4. åˆ‡æ¢æ¨¡å‹ / Switch model
set_default_model("qwen3:0.6b", scope="lesson")
```

### é«˜çº§ä½¿ç”¨ / Advanced Usage

```python
from helper_functions import (
    get_llm_response,
    get_available_models,
    test_llm_connection,
    setup_logging
)

# 1. å¯ç”¨è°ƒè¯•æ—¥å¿— / Enable debug logging
setup_logging(level='DEBUG')

# 2. æ£€æŸ¥è¿æ¥çŠ¶æ€ / Check connection status
if not test_llm_connection():
    print("âŒ æ— æ³•è¿æ¥åˆ°LLMæœåŠ¡ / Cannot connect to LLM service")
    exit(1)

# 3. è·å–å¹¶å¤„ç†å“åº” / Get and process response
prompt = "Explain the difference between list and tuple in Python."
response = get_llm_response(prompt)

# 4. åˆ†æå“åº” / Analyze response
if len(response) > 100:
    print("âœ… è·å¾—äº†è¯¦ç»†çš„å“åº” / Received detailed response")
else:
    print("âš ï¸ å“åº”è¾ƒçŸ­ï¼Œå¯èƒ½éœ€è¦æ›´å…·ä½“çš„æç¤º / Response is short, may need more specific prompt")

# 5. åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹ / List all available models
models = get_available_models()
print(f"æ‰¾åˆ° {len(models)} ä¸ªå¯ç”¨æ¨¡å‹ / Found {len(models)} available models")
```

### é”™è¯¯å¤„ç†ç¤ºä¾‹ / Error Handling Example

```python
from helper_functions import print_llm_response, get_llm_response

def safe_llm_call(prompt):
    """å®‰å…¨çš„LLMè°ƒç”¨å‡½æ•° / Safe LLM call function"""
    try:
        response = get_llm_response(prompt)
        if response and "Error" not in response:
            return response
        else:
            return "âŒ LLMè°ƒç”¨å¤±è´¥ / LLM call failed"
    except Exception as e:
        return f"âŒ å¼‚å¸¸é”™è¯¯ / Exception error: {str(e)}"

# ä½¿ç”¨å®‰å…¨è°ƒç”¨ / Use safe call
result = safe_llm_call("What is machine learning?")
print(result)
```

## é”™è¯¯å¤„ç† / Error Handling

### å¸¸è§é”™è¯¯ç±»å‹ / Common Error Types

#### 1. æ¨¡å‹æœªæ‰¾åˆ° / Model Not Found

```python
# é”™è¯¯ä¿¡æ¯ç¤ºä¾‹ / Error message example
# "Error: model 'xxx' not found"

# è§£å†³æ–¹æ¡ˆ / Solution
from helper_functions import get_available_models, set_default_model

# æ£€æŸ¥å¯ç”¨æ¨¡å‹ / Check available models
models = get_available_models()
if models:
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹ / Use first available model
    set_default_model(models[0])
else:
    print("è¯·å…ˆä¸‹è½½æ¨¡å‹ / Please download models first")
```

#### 2. è¿æ¥è¶…æ—¶ / Connection Timeout

```python
# é”™è¯¯ä¿¡æ¯ç¤ºä¾‹ / Error message example
# "Connection timeout" or "Service unavailable"

# è§£å†³æ–¹æ¡ˆ / Solution
from helper_functions import test_llm_connection
import time

def wait_for_connection(max_attempts=5):
    """ç­‰å¾…è¿æ¥å»ºç«‹ / Wait for connection to establish"""
    for attempt in range(max_attempts):
        if test_llm_connection():
            return True
        print(f"è¿æ¥å°è¯• {attempt + 1}/{max_attempts} / Connection attempt {attempt + 1}/{max_attempts}")
        time.sleep(2)
    return False

if not wait_for_connection():
    print("âŒ æ— æ³•å»ºç«‹è¿æ¥ï¼Œè¯·æ£€æŸ¥ollamaæœåŠ¡ / Cannot establish connection, please check ollama service")
```

#### 3. é…ç½®æ–‡ä»¶é”™è¯¯ / Configuration File Error

```python
# é”™è¯¯ä¿¡æ¯ç¤ºä¾‹ / Error message example
# "Failed to save config" or "Invalid JSON"

# è§£å†³æ–¹æ¡ˆ / Solution
import json
import os

def fix_config_file(config_path="ollama_config.json"):
    """ä¿®å¤é…ç½®æ–‡ä»¶ / Fix configuration file"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ / Check if file exists
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                json.load(f)  # éªŒè¯JSONæ ¼å¼ / Validate JSON format
        else:
            # åˆ›å»ºé»˜è®¤é…ç½® / Create default configuration
            default_config = {"default_model": "gemma3n:latest"}
            with open(config_path, 'w') as f:
                json.dump(default_config, f, indent=2)
        print("âœ… é…ç½®æ–‡ä»¶æ­£å¸¸ / Configuration file is OK")
    except json.JSONDecodeError:
        print("âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ / Configuration file format error")
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶é”™è¯¯ / Configuration file error: {e}")
```

### è°ƒè¯•æŠ€å·§ / Debugging Tips

#### 1. å¯ç”¨è¯¦ç»†æ—¥å¿— / Enable Verbose Logging

```python
from helper_functions import setup_logging

# å¯ç”¨è°ƒè¯•æ—¥å¿— / Enable debug logging
setup_logging(level='DEBUG')

# ç°åœ¨æ‰€æœ‰æ“ä½œéƒ½ä¼šæœ‰è¯¦ç»†æ—¥å¿— / Now all operations will have detailed logs
```

#### 2. ç³»ç»Ÿè¯Šæ–­ / System Diagnosis

```python
from helper_functions import show_model_info, test_llm_connection, get_available_models

def diagnose_system():
    """ç³»ç»Ÿè¯Šæ–­å‡½æ•° / System diagnosis function"""
    print("ğŸ” ç³»ç»Ÿè¯Šæ–­å¼€å§‹ / System diagnosis started")
    print("=" * 50)
    
    # 1. æ£€æŸ¥é…ç½® / Check configuration
    print("ğŸ“‹ æ¨¡å‹é…ç½®ä¿¡æ¯ / Model configuration info:")
    show_model_info()
    
    # 2. æµ‹è¯•è¿æ¥ / Test connection
    print("\nğŸ”— è¿æ¥æµ‹è¯• / Connection test:")
    if test_llm_connection():
        print("âœ… è¿æ¥æ­£å¸¸ / Connection OK")
    else:
        print("âŒ è¿æ¥å¤±è´¥ / Connection failed")
    
    # 3. æ£€æŸ¥å¯ç”¨æ¨¡å‹ / Check available models
    print("\nğŸ“¦ å¯ç”¨æ¨¡å‹ / Available models:")
    models = get_available_models()
    if models:
        for i, model in enumerate(models, 1):
            print(f"  {i}. {model}")
    else:
        print("  âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹ / No models found")
    
    print("=" * 50)
    print("ğŸ” ç³»ç»Ÿè¯Šæ–­å®Œæˆ / System diagnosis completed")

# è¿è¡Œè¯Šæ–­ / Run diagnosis
diagnose_system()
```

## æœ€ä½³å®è·µ / Best Practices

### 1. æ¨¡å‹é€‰æ‹© / Model Selection

```python
from helper_functions import get_available_models, set_default_model

def choose_optimal_model():
    """é€‰æ‹©æœ€ä¼˜æ¨¡å‹ / Choose optimal model"""
    models = get_available_models()
    
    # ä¼˜å…ˆçº§é¡ºåº / Priority order
    preferred_models = [
        "qwen3:0.6b",      # å¿«é€Ÿå“åº” / Fast response
        "gemma3n:latest",   # å¹³è¡¡æ€§èƒ½ / Balanced performance
        "llama3.1:8b"       # é«˜è´¨é‡ / High quality
    ]
    
    for preferred in preferred_models:
        if preferred in models:
            set_default_model(preferred)
            print(f"âœ… é€‰æ‹©æ¨¡å‹ / Selected model: {preferred}")
            return preferred
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é¦–é€‰æ¨¡å‹ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„ / If no preferred model found, use first available
    if models:
        set_default_model(models[0])
        return models[0]
    
    return None
```

### 2. æ‰¹é‡å¤„ç† / Batch Processing

```python
from helper_functions import get_llm_response

def batch_process_prompts(prompts):
    """æ‰¹é‡å¤„ç†æç¤ºè¯ / Batch process prompts"""
    results = []
    
    for i, prompt in enumerate(prompts, 1):
        print(f"å¤„ç†ç¬¬ {i}/{len(prompts)} ä¸ªæç¤º / Processing prompt {i}/{len(prompts)}")
        
        try:
            response = get_llm_response(prompt)
            results.append({
                'prompt': prompt,
                'response': response,
                'status': 'success'
            })
        except Exception as e:
            results.append({
                'prompt': prompt,
                'response': None,
                'status': 'error',
                'error': str(e)
            })
    
    return results

# ä½¿ç”¨ç¤ºä¾‹ / Usage example
prompts = [
    "What is Python?",
    "Explain variables in programming.",
    "How does machine learning work?"
]

results = batch_process_prompts(prompts)
```

### 3. æ€§èƒ½ç›‘æ§ / Performance Monitoring

```python
import time
from helper_functions import get_llm_response

def monitored_llm_call(prompt):
    """å¸¦æ€§èƒ½ç›‘æ§çš„LLMè°ƒç”¨ / LLM call with performance monitoring"""
    start_time = time.time()
    
    try:
        response = get_llm_response(prompt)
        end_time = time.time()
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡ / Calculate performance metrics
        response_time = end_time - start_time
        response_length = len(response)
        
        print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡ / Performance Stats:")
        print(f"  å“åº”æ—¶é—´ / Response time: {response_time:.2f}s")
        print(f"  å“åº”é•¿åº¦ / Response length: {response_length} chars")
        print(f"  å¤„ç†é€Ÿåº¦ / Processing speed: {response_length/response_time:.1f} chars/s")
        
        return response
    
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤±è´¥ / Call failed: {e}")
        return None
```

## ç‰ˆæœ¬å…¼å®¹æ€§ / Version Compatibility

### æ”¯æŒçš„Pythonç‰ˆæœ¬ / Supported Python Versions

- Python 3.9+
- Python 3.10+
- Python 3.11+
- Python 3.12+

### ä¾èµ–è¦æ±‚ / Dependencies

- `subprocess` (å†…ç½®æ¨¡å— / Built-in module)
- `json` (å†…ç½®æ¨¡å— / Built-in module)
- `os` (å†…ç½®æ¨¡å— / Built-in module)
- `sys` (å†…ç½®æ¨¡å— / Built-in module)

### å¤–éƒ¨ä¾èµ– / External Dependencies

- **Ollama**: éœ€è¦å®‰è£…å¹¶è¿è¡ŒollamaæœåŠ¡ / Requires ollama service to be installed and running
- **Models**: éœ€è¦ä¸‹è½½è‡³å°‘ä¸€ä¸ªollamaæ¨¡å‹ / Requires at least one ollama model to be downloaded

## è´¡çŒ®æŒ‡å— / Contributing Guidelines

å¦‚æœæ‚¨æƒ³ä¸ºAPIæ·»åŠ æ–°åŠŸèƒ½æˆ–ä¿®å¤bugï¼Œè¯·éµå¾ªä»¥ä¸‹æŒ‡å—ï¼š

If you want to add new features or fix bugs for the API, please follow these guidelines:

1. **ä¿æŒå‘åå…¼å®¹æ€§** / Maintain backward compatibility
2. **æ·»åŠ åŒè¯­æ³¨é‡Š** / Add bilingual comments
3. **ç¼–å†™å•å…ƒæµ‹è¯•** / Write unit tests
4. **æ›´æ–°æ–‡æ¡£** / Update documentation
5. **éµå¾ªä»£ç é£æ ¼** / Follow code style guidelines

## è·å–å¸®åŠ© / Getting Help

å¦‚æœæ‚¨åœ¨ä½¿ç”¨APIæ—¶é‡åˆ°é—®é¢˜ï¼š

If you encounter issues while using the API:

1. **æŸ¥çœ‹é”™è¯¯å¤„ç†éƒ¨åˆ†** / Check the error handling section
2. **è¿è¡Œç³»ç»Ÿè¯Šæ–­** / Run system diagnosis
3. **æ£€æŸ¥ollamaæœåŠ¡çŠ¶æ€** / Check ollama service status
4. **åœ¨GitHubä¸Šæäº¤Issue** / Submit an issue on GitHub

---

**æ–‡æ¡£ç‰ˆæœ¬ / Document Version**: v1.0.0  
**æœ€åæ›´æ–° / Last Updated**: 2025-01-18  
**ç»´æŠ¤è€… / Maintainer**: AI Python Bilingual Course Team