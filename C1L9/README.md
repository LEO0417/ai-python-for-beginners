# C1L9: Building LLM prompts with variables
# ç¬¬9è¯¾ï¼šä½¿ç”¨å˜é‡æ„å»ºLLMæç¤ºè¯

## ğŸ“‹ è¯¾ç¨‹æ¦‚è¿° / Course Overview

æœ¬è¯¾ç¨‹å°†æ•™ä½ å¦‚ä½•ä½¿ç”¨Pythonå˜é‡ä¸å¤§è¯­è¨€æ¨¡å‹(LLM)è¿›è¡Œäº¤äº’ï¼Œå­¦ä¹ å¦‚ä½•æ„å»ºåŠ¨æ€çš„æç¤ºè¯å¹¶è·å¾—æ™ºèƒ½å“åº”ã€‚

This lesson teaches you how to use Python variables to interact with Large Language Models (LLMs), learning to build dynamic prompts and get intelligent responses.

## ğŸš€ æœ¬åœ°æ¨¡å‹è®¾ç½® / Local Model Setup

### ğŸ¯ æ¨èé…ç½® / Recommended Configuration

å¯¹äºC1L9è¯¾ç¨‹ï¼Œæˆ‘ä»¬æ¨èä»¥ä¸‹æ¨¡å‹é…ç½®ï¼š

For C1L9 lesson, we recommend the following model configuration:

| ä½¿ç”¨åœºæ™¯ / Use Case | æ¨èæ¨¡å‹ / Recommended Model | åŸå›  / Reason |
|-------------------|----------------------------|---------------|
| åˆå­¦è€…/å¿«é€Ÿæµ‹è¯• / Beginners/Quick Testing | `qwen3:0.6b` | å“åº”å¿«é€Ÿï¼Œèµ„æºå ç”¨ä½ / Fast response, low resource usage |
| æ—¥å¸¸å­¦ä¹  / Daily Learning | `gemma3n:latest` | å¹³è¡¡æ€§èƒ½ä¸è´¨é‡ / Balanced performance and quality |
| é«˜è´¨é‡ç»ƒä¹  / High-quality Practice | `llama3.1:8b` | æœ€ä½³å›ç­”è´¨é‡ / Best response quality |

### ğŸ”§ å¿«é€Ÿè®¾ç½®æ­¥éª¤ / Quick Setup Steps

#### 1. æ£€æŸ¥ç³»ç»ŸçŠ¶æ€ / Check System Status

```bash
# æ£€æŸ¥ollamaæ˜¯å¦å·²å®‰è£… / Check if ollama is installed
which ollama

# æŸ¥çœ‹å·²å®‰è£…çš„æ¨¡å‹ / View installed models
ollama list

# æ£€æŸ¥ollamaæœåŠ¡çŠ¶æ€ / Check ollama service status
ollama serve --help
```

#### 2. ä¸‹è½½æ¨èæ¨¡å‹ / Download Recommended Model

```bash
# æ–¹æ¡ˆ1ï¼šå°å‹å¿«é€Ÿæ¨¡å‹ (æ¨èæ–°æ‰‹) / Option 1: Small fast model (recommended for beginners)
ollama pull qwen3:0.6b

# æ–¹æ¡ˆ2ï¼šä¸­ç­‰æ€§èƒ½æ¨¡å‹ / Option 2: Medium performance model
ollama pull gemma3n:latest

# æ–¹æ¡ˆ3ï¼šé«˜æ€§èƒ½æ¨¡å‹ / Option 3: High performance model
ollama pull llama3.1:8b
```

#### 3. é…ç½®è¯¾ç¨‹æ¨¡å‹ / Configure Course Model

**æ–¹æ³•1ï¼šä½¿ç”¨è¯¾ç¨‹çº§é…ç½®æ–‡ä»¶ / Method 1: Course-level Configuration**

```bash
cd C1L9

# åˆ›å»ºè¯¾ç¨‹ä¸“ç”¨é…ç½® / Create course-specific config
echo '{
  "default_model": "qwen3:0.6b"
}' > ollama_config.json
```

**æ–¹æ³•2ï¼šä½¿ç”¨Pythonè®¾ç½® / Method 2: Python Configuration**

```python
from helper_functions import set_default_model

# è®¾ç½®ä¸ºå¿«é€Ÿæ¨¡å‹ / Set to fast model
set_default_model("qwen3:0.6b", scope="lesson")

# æˆ–è®¾ç½®ä¸ºé«˜è´¨é‡æ¨¡å‹ / Or set to high-quality model
# set_default_model("llama3.1:8b", scope="lesson")
```

#### 4. éªŒè¯è®¾ç½® / Verify Setup

```python
from helper_functions import print_llm_response, get_default_model, show_model_info

# æ£€æŸ¥å½“å‰é…ç½® / Check current configuration
print(f"å½“å‰æ¨¡å‹ / Current model: {get_default_model()}")

# è¿è¡Œæµ‹è¯• / Run test
print_llm_response("Hello! Please respond to confirm the connection is working.")

# æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ / View detailed information
show_model_info()
```

### ğŸ§ª è¯¾ç¨‹åŠŸèƒ½æµ‹è¯• / Course Function Testing

è¿è¡Œä»¥ä¸‹ä»£ç æµ‹è¯•è¯¾ç¨‹ä¸­çš„ä¸»è¦åŠŸèƒ½ï¼š

Run the following code to test the main functions in the course:

```python
# æµ‹è¯•åŸºç¡€åŠŸèƒ½ / Test basic functionality
from helper_functions import print_llm_response

# æµ‹è¯•1ï¼šç®€å•é—®ç­” / Test 1: Simple Q&A
print("=== æµ‹è¯•1ï¼šç®€å•é—®ç­” / Test 1: Simple Q&A ===")
print_llm_response("What is the capital of France?")

# æµ‹è¯•2ï¼šå˜é‡æ’å€¼ / Test 2: Variable interpolation
print("\n=== æµ‹è¯•2ï¼šå˜é‡æ’å€¼ / Test 2: Variable Interpolation ===")
name = "Otto Matic"
dog_age = 21/7
print_llm_response(f"""If {name} were a dog, he would be {dog_age} years old.
Describe what life stage that would be for a dog and what that might 
entail in terms of energy level, interests, and behavior.""")

# æµ‹è¯•3ï¼šåˆ›æ„å†™ä½œ / Test 3: Creative writing
print("\n=== æµ‹è¯•3ï¼šåˆ›æ„å†™ä½œ / Test 3: Creative Writing ===")
driver = "unicorn"
drivers_vehicle = "colorful, asymmetric dinosaur car"
favorite_planet = "Pluto"
print_llm_response(f"""Write me a 300 word children's story about a {driver} racing
a {drivers_vehicle} for the {favorite_planet} champion cup.""")
```

### ğŸ” æ•…éšœæ’é™¤ / Troubleshooting

#### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ / Common Issues and Solutions

1. **æ¨¡å‹æœªæ‰¾åˆ°é”™è¯¯ / Model Not Found Error**
   
   **é”™è¯¯ä¿¡æ¯ / Error Message**: "Error: model 'xxx' not found"
   
   **è§£å†³æ–¹æ¡ˆ / Solution**:
   ```bash
   # æŸ¥çœ‹å¯ç”¨æ¨¡å‹ / Check available models
   ollama list
   
   # ä¸‹è½½ç¼ºå¤±çš„æ¨¡å‹ / Download missing model
   ollama pull qwen3:0.6b
   
   # æˆ–æ›´æ–°é…ç½®æ–‡ä»¶ä½¿ç”¨å·²æœ‰æ¨¡å‹ / Or update config to use existing model
   ```

2. **è¿æ¥è¶…æ—¶ / Connection Timeout**
   
   **é”™è¯¯ä¿¡æ¯ / Error Message**: "Connection timeout" æˆ– "Service unavailable"
   
   **è§£å†³æ–¹æ¡ˆ / Solution**:
   ```bash
   # å¯åŠ¨ollamaæœåŠ¡ / Start ollama service
   ollama serve
   
   # æˆ–åœ¨åå°è¿è¡Œ / Or run in background
   nohup ollama serve > /dev/null 2>&1 &
   ```

3. **æƒé™é—®é¢˜ / Permission Issues**
   
   **é”™è¯¯ä¿¡æ¯ / Error Message**: "Permission denied"
   
   **è§£å†³æ–¹æ¡ˆ / Solution**:
   ```bash
   # ä¿®å¤ollamaç›®å½•æƒé™ / Fix ollama directory permissions
   sudo chown -R $USER ~/.ollama
   
   # æˆ–é‡æ–°å®‰è£…ollama / Or reinstall ollama
   curl -fsSL https://ollama.ai/install.sh | sh
   ```

4. **å¯¼å…¥é”™è¯¯ / Import Error**
   
   **é”™è¯¯ä¿¡æ¯ / Error Message**: "ModuleNotFoundError: No module named 'helper_functions'"
   
   **è§£å†³æ–¹æ¡ˆ / Solution**:
   ```bash
   # ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½• / Ensure in correct directory
   cd C1L9
   
   # å®‰è£…ä¾èµ– / Install dependencies
   pip install -r requirements.txt
   
   # æ£€æŸ¥Pythonè·¯å¾„ / Check Python path
   python -c "import sys; print(sys.path)"
   ```

#### è¯Šæ–­å·¥å…· / Diagnostic Tools

è¿è¡Œè¯Šæ–­è„šæœ¬è·å–è¯¦ç»†çš„ç³»ç»Ÿä¿¡æ¯ï¼š

Run diagnostic script to get detailed system information:

```python
from helper_functions import show_model_info, test_llm_connection, get_available_models

print("ğŸ” C1L9 è¯Šæ–­æŠ¥å‘Š / C1L9 Diagnostic Report")
print("=" * 50)

# æ˜¾ç¤ºé…ç½®ä¿¡æ¯ / Show configuration info
show_model_info()

print("\nğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨ / Available Models List:")
models = get_available_models()
for i, model in enumerate(models, 1):
    print(f"  {i}. {model}")

print(f"\nğŸ”— è¿æ¥æµ‹è¯• / Connection Test:")
connection_status = test_llm_connection()
status_text = "âœ… æˆåŠŸ / Success" if connection_status else "âŒ å¤±è´¥ / Failed"
print(f"  çŠ¶æ€ / Status: {status_text}")

if not connection_status:
    print("\nğŸ’¡ å»ºè®® / Suggestions:")
    print("  1. æ£€æŸ¥ollamaæœåŠ¡æ˜¯å¦è¿è¡Œ / Check if ollama service is running")
    print("  2. éªŒè¯æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½ / Verify model is downloaded")
    print("  3. æ£€æŸ¥é…ç½®æ–‡ä»¶ / Check configuration file")
```

### ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®® / Performance Optimization Tips

#### æ¨¡å‹é€‰æ‹©ç­–ç•¥ / Model Selection Strategy

```python
# æ ¹æ®ä½ çš„ç¡¬ä»¶é…ç½®é€‰æ‹©æ¨¡å‹ / Choose model based on your hardware

import psutil
import os

def recommend_model():
    # è·å–ç³»ç»Ÿå†…å­˜ / Get system memory
    memory_gb = psutil.virtual_memory().total / (1024**3)
    
    # è·å–CPUæ ¸å¿ƒæ•° / Get CPU cores
    cpu_cores = os.cpu_count()
    
    print(f"ç³»ç»Ÿå†…å­˜ / System Memory: {memory_gb:.1f} GB")
    print(f"CPUæ ¸å¿ƒ / CPU Cores: {cpu_cores}")
    
    if memory_gb >= 16 and cpu_cores >= 8:
        print("æ¨èæ¨¡å‹ / Recommended Model: llama3.1:8b (é«˜æ€§èƒ½)")
    elif memory_gb >= 8 and cpu_cores >= 4:
        print("æ¨èæ¨¡å‹ / Recommended Model: gemma3n:latest (å¹³è¡¡)")
    else:
        print("æ¨èæ¨¡å‹ / Recommended Model: qwen3:0.6b (è½»é‡)")

recommend_model()
```

#### è¯¾ç¨‹å­¦ä¹ å»ºè®® / Course Learning Tips

1. **ä»å°æ¨¡å‹å¼€å§‹ / Start with Small Models**
   - ä½¿ç”¨ `qwen3:0.6b` å…ˆç†Ÿæ‚‰åŠŸèƒ½
   - ç¡®ä¿æ‰€æœ‰ä»£ç éƒ½èƒ½æ­£å¸¸è¿è¡Œ
   - ç†è§£å˜é‡æ’å€¼çš„æ¦‚å¿µ

2. **é€æ­¥å‡çº§ / Gradual Upgrade**
   - ç†Ÿæ‚‰åå¯ä»¥å°è¯• `gemma3n:latest`
   - æ¯”è¾ƒä¸åŒæ¨¡å‹çš„å“åº”è´¨é‡
   - è§‚å¯Ÿå“åº”æ—¶é—´çš„å·®å¼‚

3. **å®éªŒä¸åŒæç¤ºè¯ / Experiment with Different Prompts**
   - å°è¯•ä¿®æ”¹è¯¾ç¨‹ä¸­çš„ç¤ºä¾‹
   - è§‚å¯Ÿå˜é‡å˜åŒ–å¯¹è¾“å‡ºçš„å½±å“
   - å­¦ä¹ å¦‚ä½•æ„å»ºæ›´å¥½çš„æç¤ºè¯

### ğŸ“š æ‰©å±•ç»ƒä¹  / Extended Exercises

å®Œæˆè¯¾ç¨‹åŸºç¡€å†…å®¹åï¼Œå¯ä»¥å°è¯•ä»¥ä¸‹æ‰©å±•ç»ƒä¹ ï¼š

After completing the basic course content, try these extended exercises:

```python
# æ‰©å±•ç»ƒä¹ 1ï¼šå¤šå˜é‡æ•…äº‹ç”Ÿæˆ / Extended Exercise 1: Multi-variable Story Generation
character = "å‹‡æ•¢çš„å°å…”å­"  # brave little rabbit
setting = "ç¥ç§˜çš„æ£®æ—"     # mysterious forest
quest = "å¯»æ‰¾å¤±è½çš„å®è—"   # search for lost treasure
obstacle = "æ™ºæ…§çš„è€é¾™"    # wise old dragon

print_llm_response(f"""åˆ›å»ºä¸€ä¸ªå…³äº{character}åœ¨{setting}ä¸­{quest}ï¼Œ
ä½†é‡åˆ°{obstacle}é˜»æŒ¡çš„å†’é™©æ•…äº‹ã€‚æ•…äº‹è¦æœ‰å¼€å¤´ã€å‘å±•ã€é«˜æ½®å’Œç»“å±€ã€‚

Create an adventure story about {character} in {setting} on a quest to {quest},
but encountering {obstacle} as an obstacle. The story should have a beginning, 
development, climax, and ending.""")

# æ‰©å±•ç»ƒä¹ 2ï¼šä¸ªæ€§åŒ–å­¦ä¹ åŠ©æ‰‹ / Extended Exercise 2: Personalized Learning Assistant
subject = "Pythonç¼–ç¨‹"
difficulty = "åˆçº§"
learning_style = "å®è·µä¸ºä¸»"
time_available = "30åˆ†é’Ÿ"

print_llm_response(f"""ä½œä¸ºä¸€ä¸ªå­¦ä¹ åŠ©æ‰‹ï¼Œè¯·ä¸ºæƒ³å­¦ä¹ {subject}çš„å­¦ç”Ÿåˆ¶å®šä¸€ä¸ª{difficulty}éš¾åº¦ã€
{learning_style}çš„{time_available}å­¦ä¹ è®¡åˆ’ã€‚

As a learning assistant, please create a {time_available} study plan for a student 
who wants to learn {subject} at {difficulty} level with a {learning_style} approach.""")
```

### ğŸ†˜ è·å¾—å¸®åŠ© / Getting Help

å¦‚æœåœ¨C1L9è¯¾ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼š

If you encounter issues in C1L9 course:

1. **æ£€æŸ¥åŸºç¡€è®¾ç½® / Check Basic Setup**
   - è¿è¡Œä¸Šè¿°è¯Šæ–­å·¥å…·
   - ç¡®è®¤ollamaæœåŠ¡çŠ¶æ€
   - éªŒè¯æ¨¡å‹æ˜¯å¦ä¸‹è½½

2. **æŸ¥çœ‹é”™è¯¯æ—¥å¿— / Check Error Logs**
   ```python
   import logging
   logging.basicConfig(level=logging.DEBUG)
   # ç„¶åé‡æ–°è¿è¡Œæœ‰é—®é¢˜çš„ä»£ç 
   ```

3. **å¯»æ±‚å¸®åŠ© / Seek Help**
   - æŸ¥çœ‹é¡¹ç›®ä¸»READMEçš„æ•…éšœæ’é™¤éƒ¨åˆ†
   - è®¿é—®ollamaå®˜æ–¹æ–‡æ¡£ï¼šhttps://ollama.ai/
   - åœ¨é¡¹ç›®ä»“åº“æäº¤Issue

---

## ğŸ¯ å­¦ä¹ ç›®æ ‡ / Learning Objectives

å®Œæˆæœ¬è¯¾ç¨‹åï¼Œä½ å°†èƒ½å¤Ÿï¼š

After completing this lesson, you will be able to:

- âœ… ç†è§£Pythonå˜é‡çš„åŸºæœ¬æ¦‚å¿µ / Understand basic Python variable concepts
- âœ… ä½¿ç”¨f-stringæ ¼å¼åŒ–å­—ç¬¦ä¸² / Use f-string for string formatting
- âœ… å°†å˜é‡æ’å…¥åˆ°LLMæç¤ºè¯ä¸­ / Insert variables into LLM prompts
- âœ… æ„å»ºåŠ¨æ€çš„AIäº¤äº’ç¨‹åº / Build dynamic AI interaction programs
- âœ… ä¿®å¤å¸¸è§çš„å˜é‡å‘½åé”™è¯¯ / Fix common variable naming errors

**å¼€å§‹ä½ çš„AIç¼–ç¨‹ä¹‹æ—…ï¼/ Start your AI programming journey!** ğŸš€