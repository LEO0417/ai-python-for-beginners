# Ollama æœ¬åœ°å¤§æ¨¡å‹è®¾ç½®å®Œæ•´æŒ‡å—
# Complete Guide to Setting Up Ollama Local LLM

## ğŸ“– æ¦‚è¿° / Overview

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ åœ¨æœ¬é¡¹ç›®ä¸­è®¾ç½®å’Œä½¿ç”¨æœ¬åœ°ollamaå¤§è¯­è¨€æ¨¡å‹ã€‚ä½œä¸ºé¡¹ç›®ç»´æŠ¤è€…ï¼Œæˆ‘ä½¿ç”¨ `qwen3:0.6b` å’Œ `gemma3n:latest` æ¨¡å‹è¿›è¡Œå¼€å‘å’Œæµ‹è¯•ï¼Œæ¨èå…¶ä»–ç”¨æˆ·ä¹Ÿä½¿ç”¨è¿™äº›æ¨¡å‹ä»¥è·å¾—æœ€ä½³ä½“éªŒã€‚

This guide will help you set up and use local ollama Large Language Models in this project. As the project maintainer, I use `qwen3:0.6b` and `gemma3n:latest` models for development and testing, and recommend other users to use these models for the best experience.

## ğŸ¯ æ¨èé…ç½® / Recommended Configuration

### ä½œè€…ä½¿ç”¨çš„é…ç½® / Author's Configuration

```json
{
  "primary_model": "qwen3:0.6b",      // ä¸»è¦ç”¨äºå¼€å‘å’Œå¿«é€Ÿæµ‹è¯•
  "secondary_model": "gemma3n:latest", // ç”¨äºé«˜è´¨é‡ç¤ºä¾‹å’Œæ¼”ç¤º
  "system": "macOS 15.0.0",
  "hardware": "MacBook Pro M-series"
}
```

### ç”¨æˆ·æ¨èé…ç½® / User Recommended Configuration

| ç”¨æˆ·ç±»å‹ / User Type | æ¨èæ¨¡å‹ / Recommended Model | å†…å­˜éœ€æ±‚ / Memory | ä¼˜åŠ¿ / Advantages |
|---------------------|---------------------------|------------------|-------------------|
| åˆå­¦è€… / Beginners | `qwen3:0.6b` | 1GB+ | å¿«é€Ÿå¯åŠ¨ï¼Œä½èµ„æºæ¶ˆè€— / Fast startup, low resource usage |
| æ—¥å¸¸ç”¨æˆ· / Regular Users | `gemma3n:latest` | 8GB+ | å¹³è¡¡çš„æ€§èƒ½å’Œè´¨é‡ / Balanced performance and quality |
| é«˜çº§ç”¨æˆ· / Advanced Users | `llama3.1:8b` | 16GB+ | æœ€ä½³å“åº”è´¨é‡ / Best response quality |

## ğŸš€ å®‰è£…æ­¥éª¤ / Installation Steps

### Step 1: å®‰è£… Ollama / Install Ollama

#### macOS (æ¨èæ–¹æ³• / Recommended)
```bash
# æ–¹æ³•1ï¼šå®˜æ–¹å®‰è£…è„šæœ¬ / Method 1: Official installer
curl -fsSL https://ollama.ai/install.sh | sh

# æ–¹æ³•2ï¼šHomebrew / Method 2: Homebrew
brew install ollama
```

#### Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Windows
1. è®¿é—® https://ollama.ai/download
2. ä¸‹è½½ Windows å®‰è£…åŒ…
3. è¿è¡Œå®‰è£…ç¨‹åº

### Step 2: éªŒè¯å®‰è£… / Verify Installation

```bash
# æ£€æŸ¥ollamaç‰ˆæœ¬ / Check ollama version
ollama --version

# æ£€æŸ¥æœåŠ¡çŠ¶æ€ / Check service status
ollama serve --help
```

### Step 3: ä¸‹è½½æ¨èæ¨¡å‹ / Download Recommended Models

#### ä½œè€…æ¨èçš„æ¨¡å‹ç»„åˆ / Author's Recommended Model Combination

```bash
# ä¸»åŠ›æ¨¡å‹ï¼šè½»é‡å¿«é€Ÿ / Primary model: Lightweight and fast
ollama pull qwen3:0.6b

# å¤‡ç”¨æ¨¡å‹ï¼šé«˜è´¨é‡ / Backup model: High quality
ollama pull gemma3n:latest

# å¯é€‰ï¼šæœ€ä½³è´¨é‡æ¨¡å‹ / Optional: Best quality model
ollama pull llama3.1:8b
```

#### ä¸‹è½½è¿›åº¦ç›‘æ§ / Download Progress Monitoring

```bash
# æŸ¥çœ‹ä¸‹è½½çŠ¶æ€ / Check download status
ollama list

# ç¤ºä¾‹è¾“å‡º / Example output:
# NAME              ID              SIZE      MODIFIED          
# qwen3:0.6b        7df6b6e09427    522 MB    About an hour ago    
# gemma3n:latest    15cb39fd9394    7.5 GB    6 days ago           
```

### Step 4: å¯åŠ¨æœåŠ¡ / Start Service

```bash
# å‰å°å¯åŠ¨ (ç”¨äºè°ƒè¯•) / Foreground start (for debugging)
ollama serve

# åå°å¯åŠ¨ (æ¨è) / Background start (recommended)
nohup ollama serve > ollama.log 2>&1 &
```

### Step 5: æµ‹è¯•æ¨¡å‹ / Test Models

```bash
# æµ‹è¯•qwen3æ¨¡å‹ / Test qwen3 model
echo "Hello, how are you?" | ollama run qwen3:0.6b

# æµ‹è¯•gemma3næ¨¡å‹ / Test gemma3n model
echo "What is Python?" | ollama run gemma3n:latest
```

## âš™ï¸ é¡¹ç›®é…ç½® / Project Configuration

### å…¨å±€é…ç½® / Global Configuration

ç¼–è¾‘é¡¹ç›®æ ¹ç›®å½•çš„ `ollama_config.json`ï¼š

Edit `ollama_config.json` in the project root:

```json
{
  "default_model": "qwen3:0.6b",
  "fallback_model": "gemma3n:latest",
  "timeout": 60,
  "retry_attempts": 3
}
```

### è¯¾ç¨‹çº§é…ç½® / Lesson-level Configuration

ä¸ºç‰¹å®šè¯¾ç¨‹è®¾ç½®ä¸åŒæ¨¡å‹ï¼š

Set different models for specific lessons:

```bash
cd C1L9
echo '{
  "default_model": "qwen3:0.6b"
}' > ollama_config.json
```

### ç¯å¢ƒå˜é‡é…ç½® / Environment Variable Configuration

```bash
# ä¸´æ—¶è®¾ç½® / Temporary setting
export OLLAMA_MODEL="qwen3:0.6b"

# æ°¸ä¹…è®¾ç½® / Permanent setting (æ·»åŠ åˆ° ~/.bashrc æˆ– ~/.zshrc)
echo 'export OLLAMA_MODEL="qwen3:0.6b"' >> ~/.zshrc
source ~/.zshrc
```

## ğŸ”§ Python é›†æˆ / Python Integration

### åŸºç¡€ä½¿ç”¨ / Basic Usage

```python
from helper_functions import print_llm_response, get_default_model

# æ£€æŸ¥å½“å‰æ¨¡å‹ / Check current model
print(f"å½“å‰æ¨¡å‹ / Current model: {get_default_model()}")

# åŸºç¡€å¯¹è¯ / Basic conversation
print_llm_response("Hello! Please introduce yourself.")

# ä½¿ç”¨å˜é‡ / Using variables
name = "Alice"
age = 25
print_llm_response(f"Tell me about a {age}-year-old person named {name}.")
```

### é«˜çº§é…ç½® / Advanced Configuration

```python
from helper_functions import (
    set_default_model, 
    get_available_models, 
    show_model_info,
    test_llm_connection
)

# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹ / View all available models
models = get_available_models()
print("å¯ç”¨æ¨¡å‹ / Available models:", models)

# åˆ‡æ¢æ¨¡å‹ / Switch model
set_default_model("gemma3n:latest")

# æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ / Show detailed info
show_model_info()

# æµ‹è¯•è¿æ¥ / Test connection
is_connected = test_llm_connection()
print(f"è¿æ¥çŠ¶æ€ / Connection status: {is_connected}")
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯” / Performance Comparison

åŸºäºä½œè€…çš„æµ‹è¯•ç»“æœï¼š

Based on author's test results:

| æ¨¡å‹ / Model | å“åº”æ—¶é—´ / Response Time | å†…å­˜ä½¿ç”¨ / Memory Usage | è´¨é‡è¯„åˆ† / Quality Score | é€‚ç”¨åœºæ™¯ / Use Case |
|--------------|------------------------|------------------------|------------------------|-------------------|
| `qwen3:0.6b` | ~2-5ç§’ / ~2-5s | ~1GB | 7/10 | å¿«é€ŸåŸå‹ã€å­¦ä¹ ç»ƒä¹  / Quick prototyping, learning |
| `gemma3n:latest` | ~10-20ç§’ / ~10-20s | ~8GB | 8.5/10 | æ—¥å¸¸å¼€å‘ã€æ¼”ç¤º / Daily development, demos |
| `llama3.1:8b` | ~15-30ç§’ / ~15-30s | ~16GB | 9.5/10 | ç”Ÿäº§ç¯å¢ƒã€é«˜è´¨é‡è¾“å‡º / Production, high-quality output |

## ğŸ” æ•…éšœæ’é™¤ / Troubleshooting

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ / Common Issues and Solutions

#### 1. æ¨¡å‹ä¸‹è½½å¤±è´¥ / Model Download Failed

**é—®é¢˜ / Issue**: "Error downloading model"

**è§£å†³æ–¹æ¡ˆ / Solution**:
```bash
# æ£€æŸ¥ç½‘ç»œè¿æ¥ / Check network connection
ping ollama.ai

# ä½¿ç”¨ä»£ç†ä¸‹è½½ / Download with proxy
export HTTP_PROXY=http://your-proxy:port
ollama pull qwen3:0.6b

# æ‰‹åŠ¨é‡è¯• / Manual retry
ollama pull qwen3:0.6b --retry 5
```

#### 2. æ¨¡å‹ä¸å­˜åœ¨é”™è¯¯ / Model Not Found Error

**é—®é¢˜ / Issue**: "Error: model 'xxx' not found"

**è§£å†³æ–¹æ¡ˆ / Solution**:
```bash
# åˆ—å‡ºå·²å®‰è£…æ¨¡å‹ / List installed models
ollama list

# ä¸‹è½½ç¼ºå¤±æ¨¡å‹ / Download missing model
ollama pull qwen3:0.6b

# æˆ–æ›´æ–°é…ç½®æ–‡ä»¶ / Or update config file
echo '{"default_model": "qwen3:0.6b"}' > ollama_config.json
```

#### 3. æœåŠ¡è¿æ¥é—®é¢˜ / Service Connection Issues

**é—®é¢˜ / Issue**: "Connection refused" æˆ– "Service unavailable"

**è§£å†³æ–¹æ¡ˆ / Solution**:
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€ / Check service status
ps aux | grep ollama

# é‡å¯æœåŠ¡ / Restart service
pkill ollama
ollama serve &

# æ£€æŸ¥ç«¯å£å ç”¨ / Check port usage
lsof -i :11434
```

#### 4. å†…å­˜ä¸è¶³ / Out of Memory

**é—®é¢˜ / Issue**: "Out of memory" æˆ–ç³»ç»Ÿå¡é¡¿

**è§£å†³æ–¹æ¡ˆ / Solution**:
```bash
# åˆ‡æ¢åˆ°æ›´å°çš„æ¨¡å‹ / Switch to smaller model
export OLLAMA_MODEL="qwen3:0.6b"

# æˆ–è€…é™åˆ¶å†…å­˜ä½¿ç”¨ / Or limit memory usage
export OLLAMA_MAX_MEMORY=4GB
```

### è¯Šæ–­å·¥å…· / Diagnostic Tools

åˆ›å»ºè¯Šæ–­è„šæœ¬ `diagnose_ollama.py`ï¼š

Create diagnostic script `diagnose_ollama.py`:

```python
#!/usr/bin/env python3
import subprocess
import json
import os
import sys

def diagnose_ollama():
    print("ğŸ” Ollama ç³»ç»Ÿè¯Šæ–­ / Ollama System Diagnosis")
    print("=" * 50)
    
    # æ£€æŸ¥ollamaå®‰è£… / Check ollama installation
    try:
        result = subprocess.run(['which', 'ollama'], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ… Ollama å·²å®‰è£… / Ollama installed: {result.stdout.strip()}")
        else:
            print("âŒ Ollama æœªå®‰è£… / Ollama not installed")
            return
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥ / Check failed: {e}")
        return
    
    # æ£€æŸ¥æœåŠ¡çŠ¶æ€ / Check service status
    try:
        result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)
        if 'ollama serve' in result.stdout:
            print("âœ… Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ / Ollama service is running")
        else:
            print("âš ï¸  Ollama æœåŠ¡æœªè¿è¡Œ / Ollama service not running")
    except Exception as e:
        print(f"âš ï¸  æ— æ³•æ£€æŸ¥æœåŠ¡çŠ¶æ€ / Cannot check service status: {e}")
    
    # æ£€æŸ¥å·²å®‰è£…æ¨¡å‹ / Check installed models
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        if result.returncode == 0:
            print("ğŸ“‹ å·²å®‰è£…æ¨¡å‹ / Installed models:")
            print(result.stdout)
        else:
            print("âŒ æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨ / Cannot get model list")
    except Exception as e:
        print(f"âŒ æ£€æŸ¥æ¨¡å‹å¤±è´¥ / Model check failed: {e}")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶ / Check config files
    config_files = ['ollama_config.json', 'C1L9/ollama_config.json']
    for config_file in config_files:
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r') as f:
                    config = json.load(f)
                print(f"ğŸ“„ é…ç½®æ–‡ä»¶ / Config file {config_file}:")
                print(f"   é»˜è®¤æ¨¡å‹ / Default model: {config.get('default_model', 'Not set')}")
            except Exception as e:
                print(f"âš ï¸  é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥ / Config file read failed: {e}")
    
    # æµ‹è¯•æ¨¡å‹å“åº” / Test model response
    try:
        from helper_functions import get_default_model, test_llm_connection
        current_model = get_default_model()
        print(f"ğŸ¯ å½“å‰æ¨¡å‹ / Current model: {current_model}")
        
        is_connected = test_llm_connection()
        status = "âœ… æ­£å¸¸ / Normal" if is_connected else "âŒ å¼‚å¸¸ / Abnormal"
        print(f"ğŸ”— è¿æ¥æµ‹è¯• / Connection test: {status}")
        
    except Exception as e:
        print(f"âŒ Pythoné›†æˆæµ‹è¯•å¤±è´¥ / Python integration test failed: {e}")

if __name__ == "__main__":
    diagnose_ollama()
```

è¿è¡Œè¯Šæ–­ï¼š
```bash
python diagnose_ollama.py
```

## ğŸ’¡ æœ€ä½³å®è·µ / Best Practices

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥ / Model Selection Strategy

```python
def choose_model_by_task(task_type):
    """æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹ / Choose best model by task type"""
    
    model_recommendations = {
        'learning': 'qwen3:0.6b',           # å­¦ä¹ ç»ƒä¹  / Learning exercises
        'development': 'gemma3n:latest',     # å¼€å‘è°ƒè¯• / Development debugging  
        'demo': 'gemma3n:latest',            # æ¼”ç¤ºå±•ç¤º / Demonstrations
        'production': 'llama3.1:8b',        # ç”Ÿäº§ç¯å¢ƒ / Production
        'creative': 'llama3.1:8b',          # åˆ›æ„å†™ä½œ / Creative writing
        'coding': 'gemma3n:latest',          # ä»£ç ç›¸å…³ / Code-related
    }
    
    return model_recommendations.get(task_type, 'qwen3:0.6b')

# ä½¿ç”¨ç¤ºä¾‹ / Usage example
task = 'learning'
recommended_model = choose_model_by_task(task)
print(f"æ¨èæ¨¡å‹ / Recommended model for {task}: {recommended_model}")
```

### 2. æ€§èƒ½ç›‘æ§ / Performance Monitoring

```python
import time
import psutil

def monitor_llm_performance(prompt):
    """ç›‘æ§LLMè°ƒç”¨æ€§èƒ½ / Monitor LLM call performance"""
    
    # è®°å½•å¼€å§‹æ—¶é—´å’Œå†…å­˜ / Record start time and memory
    start_time = time.time()
    start_memory = psutil.virtual_memory().used / (1024**3)
    
    # è°ƒç”¨LLM / Call LLM
    response = get_llm_response(prompt)
    
    # è®°å½•ç»“æŸæ—¶é—´å’Œå†…å­˜ / Record end time and memory
    end_time = time.time()
    end_memory = psutil.virtual_memory().used / (1024**3)
    
    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡ / Calculate performance metrics
    response_time = end_time - start_time
    memory_used = end_memory - start_memory
    
    print(f"å“åº”æ—¶é—´ / Response time: {response_time:.2f}s")
    print(f"å†…å­˜ä½¿ç”¨ / Memory used: {memory_used:.2f}GB")
    print(f"å“åº”é•¿åº¦ / Response length: {len(response)} chars")
    
    return response
```

### 3. é…ç½®ç®¡ç† / Configuration Management

```python
class OllamaConfigManager:
    """Ollamaé…ç½®ç®¡ç†å™¨ / Ollama Configuration Manager"""
    
    def __init__(self):
        self.global_config = "ollama_config.json"
        self.lesson_configs = {}
    
    def set_lesson_model(self, lesson, model):
        """ä¸ºç‰¹å®šè¯¾ç¨‹è®¾ç½®æ¨¡å‹ / Set model for specific lesson"""
        lesson_dir = f"{lesson}"
        config_path = f"{lesson_dir}/ollama_config.json"
        
        config = {"default_model": model}
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"âœ… {lesson} æ¨¡å‹è®¾ç½®ä¸º / Model set to: {model}")
    
    def backup_config(self):
        """å¤‡ä»½å½“å‰é…ç½® / Backup current configuration"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        backup_file = f"ollama_config_backup_{timestamp}.json"
        
        if os.path.exists(self.global_config):
            with open(self.global_config, 'r') as src:
                with open(backup_file, 'w') as dst:
                    dst.write(src.read())
            print(f"âœ… é…ç½®å·²å¤‡ä»½åˆ° / Config backed up to: {backup_file}")

# ä½¿ç”¨ç¤ºä¾‹ / Usage example
config_manager = OllamaConfigManager()
config_manager.set_lesson_model("C1L9", "qwen3:0.6b")
config_manager.backup_config()
```

## ğŸš€ é«˜çº§åŠŸèƒ½ / Advanced Features

### æ¨¡å‹çƒ­åˆ‡æ¢ / Hot Model Switching

```python
def switch_model_dynamically(task_complexity):
    """æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€åˆ‡æ¢æ¨¡å‹ / Dynamically switch model based on task complexity"""
    
    if task_complexity == 'simple':
        model = 'qwen3:0.6b'
    elif task_complexity == 'medium':
        model = 'gemma3n:latest'
    else:  # complex
        model = 'llama3.1:8b'
    
    set_default_model(model)
    print(f"å·²åˆ‡æ¢åˆ°æ¨¡å‹ / Switched to model: {model}")
    return model

# ä½¿ç”¨ç¤ºä¾‹ / Usage example
switch_model_dynamically('simple')
print_llm_response("What is Python?")

switch_model_dynamically('complex')
print_llm_response("Explain quantum computing in detail.")
```

### å¹¶å‘æ¨¡å‹è°ƒç”¨ / Concurrent Model Calls

```python
import asyncio
import concurrent.futures

async def compare_models(prompt):
    """æ¯”è¾ƒä¸åŒæ¨¡å‹çš„å“åº” / Compare responses from different models"""
    
    models = ['qwen3:0.6b', 'gemma3n:latest']
    responses = {}
    
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = {
            model: executor.submit(get_model_response, prompt, model)
            for model in models
        }
        
        for model, future in futures.items():
            try:
                response = future.result(timeout=60)
                responses[model] = response
            except Exception as e:
                responses[model] = f"Error: {e}"
    
    return responses

def get_model_response(prompt, model):
    """è·å–æŒ‡å®šæ¨¡å‹çš„å“åº” / Get response from specified model"""
    original_model = get_default_model()
    try:
        set_default_model(model)
        return get_llm_response(prompt)
    finally:
        set_default_model(original_model)
```

## ğŸ“š ç”¨æˆ·æŒ‡å— / User Guide

### æ–°ç”¨æˆ·å¿«é€Ÿå¼€å§‹ / Quick Start for New Users

1. **æŒ‰ç…§æœ¬æŒ‡å—å®‰è£…ollamaå’Œæ¨¡å‹**
2. **å…‹éš†é¡¹ç›®å¹¶è¿›å…¥ç›®å½•**
3. **è¿è¡Œè¯Šæ–­è„šæœ¬éªŒè¯è®¾ç½®**
4. **ä»C1L9è¯¾ç¨‹å¼€å§‹å­¦ä¹ **

### è´¡çŒ®è€…æŒ‡å— / Contributor Guide

å¦‚æœä½ æƒ³ä¸ºé¡¹ç›®è´¡çŒ®ä»£ç ï¼š

If you want to contribute to the project:

1. **ä½¿ç”¨ä½œè€…æ¨èçš„æ¨¡å‹é…ç½®è¿›è¡Œæµ‹è¯•**
2. **ç¡®ä¿ä½ çš„æ›´æ”¹ä¸ç°æœ‰çš„æ¨¡å‹å…¼å®¹**
3. **åœ¨PRä¸­è¯´æ˜ä½ ä½¿ç”¨çš„æ¨¡å‹ç‰ˆæœ¬**
4. **æä¾›æ€§èƒ½æµ‹è¯•ç»“æœ**

### é—®é¢˜æŠ¥å‘Š / Issue Reporting

æŠ¥å‘Šé—®é¢˜æ—¶è¯·åŒ…å«ï¼š

When reporting issues, please include:

- ç³»ç»Ÿä¿¡æ¯ (`uname -a`)
- Ollamaç‰ˆæœ¬ (`ollama --version`)
- æ¨¡å‹åˆ—è¡¨ (`ollama list`)
- è¯Šæ–­è„šæœ¬è¾“å‡º
- é”™è¯¯è¯¦ç»†ä¿¡æ¯

## ğŸ¤ ç¤¾åŒºå’Œæ”¯æŒ / Community and Support

- **GitHub Issues**: æŠ¥å‘Šé—®é¢˜å’ŒåŠŸèƒ½è¯·æ±‚ / Report issues and feature requests
- **Ollamaå®˜æ–¹æ–‡æ¡£**: https://ollama.ai/
- **é¡¹ç›®è®¨è®º**: åœ¨é¡¹ç›®ä»“åº“çš„DiscussionsåŒºåŸŸ / In project repository Discussions

## ğŸ“„ ç‰ˆæœ¬å†å² / Version History

- **v1.0**: åˆå§‹ç‰ˆæœ¬ï¼Œæ”¯æŒåŸºç¡€ollamaé›†æˆ / Initial version with basic ollama integration
- **v1.1**: æ·»åŠ æ™ºèƒ½æ¨¡å‹é€‰æ‹©å’Œæ•…éšœè½¬ç§» / Added smart model selection and fallback
- **v1.2**: å¢å¼ºé…ç½®ç®¡ç†å’Œè¯Šæ–­å·¥å…· / Enhanced configuration management and diagnostic tools

---

**ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼/ Happy coding!** ğŸš€

å¦‚æœ‰é—®é¢˜ï¼Œè¯·éšæ—¶åœ¨é¡¹ç›®ä»“åº“ä¸­æå‡ºIssueã€‚

If you have any questions, feel free to create an issue in the project repository. 