# æ•…éšœæ’é™¤æŒ‡å— / Troubleshooting Guide

æœ¬æŒ‡å—æä¾›äº†AI Pythonåˆå­¦è€…æ•™ç¨‹é¡¹ç›®ä¸­å¸¸è§é—®é¢˜çš„è¯¦ç»†è§£å†³æ–¹æ¡ˆã€‚

This guide provides detailed solutions for common issues in the AI Python for Beginners tutorial project.

## ç›®å½• / Table of Contents

1. [å¿«é€Ÿè¯Šæ–­ / Quick Diagnosis](#å¿«é€Ÿè¯Šæ–­--quick-diagnosis)
2. [å®‰è£…é—®é¢˜ / Installation Issues](#å®‰è£…é—®é¢˜--installation-issues)
3. [é…ç½®é—®é¢˜ / Configuration Issues](#é…ç½®é—®é¢˜--configuration-issues)
4. [æ¨¡å‹é—®é¢˜ / Model Issues](#æ¨¡å‹é—®é¢˜--model-issues)
5. [è¿æ¥é—®é¢˜ / Connection Issues](#è¿æ¥é—®é¢˜--connection-issues)
6. [æ€§èƒ½é—®é¢˜ / Performance Issues](#æ€§èƒ½é—®é¢˜--performance-issues)
7. [å¯¼å…¥é”™è¯¯ / Import Errors](#å¯¼å…¥é”™è¯¯--import-errors)
8. [ç³»ç»Ÿå…¼å®¹æ€§ / System Compatibility](#ç³»ç»Ÿå…¼å®¹æ€§--system-compatibility)
9. [é«˜çº§æ•…éšœæ’é™¤ / Advanced Troubleshooting](#é«˜çº§æ•…éšœæ’é™¤--advanced-troubleshooting)

## å¿«é€Ÿè¯Šæ–­ / Quick Diagnosis

### ä¸€é”®è¯Šæ–­è„šæœ¬ / One-Click Diagnosis Script

åœ¨å¼€å§‹æ•…éšœæ’é™¤ä¹‹å‰ï¼Œè¿è¡Œä»¥ä¸‹è¯Šæ–­è„šæœ¬ï¼š

Before starting troubleshooting, run this diagnostic script:

```python
from helper_functions import show_model_info, test_llm_connection, get_available_models
import subprocess
import sys
import os

def quick_diagnosis():
    """å¿«é€Ÿç³»ç»Ÿè¯Šæ–­ / Quick system diagnosis"""
    print("ğŸ” AI Pythonè¯¾ç¨‹ - å¿«é€Ÿè¯Šæ–­ / AI Python Course - Quick Diagnosis")
    print("=" * 60)
    
    # 1. Pythonç¯å¢ƒæ£€æŸ¥ / Python environment check
    print(f"ğŸ Pythonç‰ˆæœ¬ / Python version: {sys.version}")
    print(f"ğŸ“ Pythonè·¯å¾„ / Python path: {sys.executable}")
    
    # 2. é¡¹ç›®è·¯å¾„æ£€æŸ¥ / Project path check
    print(f"ğŸ“ å½“å‰å·¥ä½œç›®å½• / Current working directory: {os.getcwd()}")
    
    # 3. Ollamaå®‰è£…æ£€æŸ¥ / Ollama installation check
    try:
        result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ… Ollamaå·²å®‰è£… / Ollama installed: {result.stdout.strip()}")
        else:
            print("âŒ Ollamaæœªæ­£ç¡®å®‰è£… / Ollama not properly installed")
    except FileNotFoundError:
        print("âŒ Ollamaæœªæ‰¾åˆ° / Ollama not found")
    
    # 4. æ¨¡å‹é…ç½®æ£€æŸ¥ / Model configuration check
    print("\nğŸ“‹ æ¨¡å‹é…ç½®ä¿¡æ¯ / Model configuration info:")
    show_model_info()
    
    # 5. è¿æ¥æµ‹è¯• / Connection test
    print(f"\nğŸ”— è¿æ¥æµ‹è¯• / Connection test:")
    if test_llm_connection():
        print("âœ… LLMè¿æ¥æ­£å¸¸ / LLM connection working")
    else:
        print("âŒ LLMè¿æ¥å¤±è´¥ / LLM connection failed")
    
    # 6. å»ºè®®ä¸‹ä¸€æ­¥ / Next steps suggestion
    print("\nğŸ’¡ å»ºè®® / Recommendations:")
    models = get_available_models()
    if not models:
        print("1. ä¸‹è½½æ¨èæ¨¡å‹ / Download recommended model: ollama pull qwen3:0.6b")
    
    print("2. æ£€æŸ¥å…·ä½“é”™è¯¯ä¿¡æ¯ / Check specific error messages")
    print("3. æŸ¥çœ‹è¯¦ç»†æ•…éšœæ’é™¤æŒ‡å— / See detailed troubleshooting guide")
    print("=" * 60)

# è¿è¡Œè¯Šæ–­ / Run diagnosis
quick_diagnosis()
```

## å®‰è£…é—®é¢˜ / Installation Issues

### é—®é¢˜1ï¼šOllamaå®‰è£…å¤±è´¥ / Issue 1: Ollama Installation Failed

**ç—‡çŠ¶ / Symptoms:**
- `ollama: command not found`
- `bash: ollama: command not found`
- ä¸‹è½½å®‰è£…è„šæœ¬å¤±è´¥

**è§£å†³æ–¹æ¡ˆ / Solutions:**

#### macOS

```bash
# æ–¹æ³•1ï¼šå®˜æ–¹å®‰è£…è„šæœ¬ / Method 1: Official install script
curl -fsSL https://ollama.ai/install.sh | sh

# æ–¹æ³•2ï¼šä½¿ç”¨Homebrew / Method 2: Using Homebrew
brew install ollama

# æ–¹æ³•3ï¼šæ‰‹åŠ¨ä¸‹è½½ / Method 3: Manual download
# è®¿é—® https://ollama.ai/download ä¸‹è½½å¯¹åº”ç‰ˆæœ¬

# éªŒè¯å®‰è£… / Verify installation
ollama --version
which ollama
```

#### Linux

```bash
# æ–¹æ³•1ï¼šå®˜æ–¹å®‰è£…è„šæœ¬ / Method 1: Official install script
curl -fsSL https://ollama.ai/install.sh | sh

# æ–¹æ³•2ï¼šä½¿ç”¨åŒ…ç®¡ç†å™¨ / Method 2: Using package manager
# Ubuntu/Debian
sudo apt update
sudo apt install ollama

# CentOS/RHEL
sudo yum install ollama

# éªŒè¯å®‰è£… / Verify installation
ollama --version
systemctl status ollama
```

#### Windows

```powershell
# æ–¹æ³•1ï¼šä»å®˜ç½‘ä¸‹è½½ / Method 1: Download from official site
# è®¿é—® https://ollama.ai/download ä¸‹è½½Windowsç‰ˆæœ¬

# æ–¹æ³•2ï¼šä½¿ç”¨Chocolatey / Method 2: Using Chocolatey
choco install ollama

# æ–¹æ³•3ï¼šä½¿ç”¨Scoop / Method 3: Using Scoop
scoop install ollama

# éªŒè¯å®‰è£… / Verify installation
ollama --version
```

### é—®é¢˜2ï¼šPythonä¾èµ–å®‰è£…å¤±è´¥ / Issue 2: Python Dependencies Installation Failed

**ç—‡çŠ¶ / Symptoms:**
- `ModuleNotFoundError: No module named 'xxx'`
- `pip install` å¤±è´¥

**è§£å†³æ–¹æ¡ˆ / Solutions:**

```bash
# 1. å‡çº§pip / Upgrade pip
python -m pip install --upgrade pip

# 2. ä½¿ç”¨requirements.txt / Use requirements.txt
cd ai-python-for-beginners
pip install -r requirements.txt

# 3. å•ç‹¬å®‰è£…ç¼ºå¤±æ¨¡å— / Install missing modules individually
pip install jupyter notebook ipython

# 4. ä½¿ç”¨condaç¯å¢ƒ / Use conda environment
conda create -n ai-python python=3.9
conda activate ai-python
pip install -r requirements.txt

# 5. æ£€æŸ¥Pythonç‰ˆæœ¬å…¼å®¹æ€§ / Check Python version compatibility
python --version  # éœ€è¦Python 3.9+ / Requires Python 3.9+
```

## é…ç½®é—®é¢˜ / Configuration Issues

### é—®é¢˜3ï¼šé…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ / Issue 3: Configuration File Format Error

**ç—‡çŠ¶ / Symptoms:**
- `JSON decode error`
- `Invalid JSON format`
- é…ç½®ä¸ç”Ÿæ•ˆ

**è§£å†³æ–¹æ¡ˆ / Solutions:**

```python
import json
import os

def fix_config_file():
    """ä¿®å¤é…ç½®æ–‡ä»¶ / Fix configuration file"""
    config_files = [
        'ollama_config.json',
        'C1L9/ollama_config.json',
        'C1L10/ollama_config.json'
    ]
    
    for config_file in config_files:
        if os.path.exists(config_file):
            try:
                # å°è¯•è¯»å–é…ç½®æ–‡ä»¶ / Try to read config file
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                print(f"âœ… {config_file} æ ¼å¼æ­£ç¡® / format is correct")
                
            except json.JSONDecodeError as e:
                print(f"âŒ {config_file} JSONæ ¼å¼é”™è¯¯ / JSON format error: {e}")
                
                # åˆ›å»ºæ ‡å‡†é…ç½® / Create standard configuration
                standard_config = {
                    "default_model": "qwen3:0.6b",
                    "timeout": 60,
                    "retry_attempts": 3
                }
                
                # å¤‡ä»½é”™è¯¯æ–‡ä»¶ / Backup error file
                backup_file = f"{config_file}.backup"
                if os.path.exists(config_file):
                    os.rename(config_file, backup_file)
                    print(f"ğŸ“„ å·²å¤‡ä»½é”™è¯¯æ–‡ä»¶åˆ° / Backed up error file to: {backup_file}")
                
                # å†™å…¥æ ‡å‡†é…ç½® / Write standard configuration
                with open(config_file, 'w', encoding='utf-8') as f:
                    json.dump(standard_config, f, indent=2, ensure_ascii=False)
                print(f"âœ… å·²ä¿®å¤é…ç½®æ–‡ä»¶ / Fixed configuration file: {config_file}")

# è¿è¡Œä¿®å¤ / Run fix
fix_config_file()
```

### é—®é¢˜4ï¼šç¯å¢ƒå˜é‡é…ç½®é—®é¢˜ / Issue 4: Environment Variable Configuration Issues

**ç—‡çŠ¶ / Symptoms:**
- ç¯å¢ƒå˜é‡ä¸ç”Ÿæ•ˆ
- æ¨¡å‹è®¾ç½®è¢«å¿½ç•¥

**è§£å†³æ–¹æ¡ˆ / Solutions:**

```bash
# 1. æ£€æŸ¥å½“å‰ç¯å¢ƒå˜é‡ / Check current environment variables
echo $OLLAMA_MODEL
echo $OLLAMA_HOST
echo $OLLAMA_PORT

# 2. è®¾ç½®ä¸´æ—¶ç¯å¢ƒå˜é‡ / Set temporary environment variables
export OLLAMA_MODEL="qwen3:0.6b"
export OLLAMA_HOST="127.0.0.1"
export OLLAMA_PORT="11434"

# 3. æ°¸ä¹…è®¾ç½®ç¯å¢ƒå˜é‡ / Set permanent environment variables
# å¯¹äºbash / For bash
echo 'export OLLAMA_MODEL="qwen3:0.6b"' >> ~/.bashrc
source ~/.bashrc

# å¯¹äºzsh / For zsh
echo 'export OLLAMA_MODEL="qwen3:0.6b"' >> ~/.zshrc
source ~/.zshrc

# 4. éªŒè¯ç¯å¢ƒå˜é‡ / Verify environment variables
python -c "import os; print('OLLAMA_MODEL:', os.getenv('OLLAMA_MODEL'))"
```

## æ¨¡å‹é—®é¢˜ / Model Issues

### é—®é¢˜5ï¼šæ¨¡å‹æœªæ‰¾åˆ° / Issue 5: Model Not Found

**ç—‡çŠ¶ / Symptoms:**
- `Error: model 'xxx' not found`
- `pull model first`

**è§£å†³æ–¹æ¡ˆ / Solutions:**

```bash
# 1. æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹ / Check installed models
ollama list

# 2. ä¸‹è½½æ¨èæ¨¡å‹ / Download recommended models
ollama pull qwen3:0.6b         # è½»é‡çº§æ¨¡å‹ / Lightweight model
ollama pull gemma3n:latest     # å¹³è¡¡æ¨¡å‹ / Balanced model
ollama pull llama3.1:8b        # é«˜æ€§èƒ½æ¨¡å‹ / High-performance model

# 3. éªŒè¯æ¨¡å‹ä¸‹è½½ / Verify model download
ollama list
ollama run qwen3:0.6b "Hello, test message"

# 4. ä½¿ç”¨Pythonæ£€æŸ¥æ¨¡å‹ / Check models using Python
from helper_functions import get_available_models, set_default_model

models = get_available_models()
print("å¯ç”¨æ¨¡å‹ / Available models:", models)

if models:
    set_default_model(models[0])  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹ / Use first available model
```

### é—®é¢˜6ï¼šæ¨¡å‹ä¸‹è½½å¤±è´¥ / Issue 6: Model Download Failed

**ç—‡çŠ¶ / Symptoms:**
- ä¸‹è½½ä¸­æ–­
- ç½‘ç»œè¿æ¥é”™è¯¯
- ç£ç›˜ç©ºé—´ä¸è¶³

**è§£å†³æ–¹æ¡ˆ / Solutions:**

```bash
# 1. æ£€æŸ¥ç£ç›˜ç©ºé—´ / Check disk space
df -h

# 2. æ£€æŸ¥ç½‘ç»œè¿æ¥ / Check network connection
ping ollama.ai
curl -I https://ollama.ai

# 3. ä½¿ç”¨ä»£ç†ä¸‹è½½ / Download with proxy
export HTTP_PROXY=http://your-proxy:port
export HTTPS_PROXY=http://your-proxy:port
ollama pull qwen3:0.6b

# 4. é‡è¯•ä¸‹è½½ / Retry download
ollama pull qwen3:0.6b --retry 3

# 5. æ¸…ç†æŸåçš„ä¸‹è½½ / Clean corrupted downloads
ollama rm qwen3:0.6b  # åˆ é™¤æŸåçš„æ¨¡å‹ / Remove corrupted model
ollama pull qwen3:0.6b  # é‡æ–°ä¸‹è½½ / Re-download
```

## è¿æ¥é—®é¢˜ / Connection Issues

### é—®é¢˜7ï¼šOllamaæœåŠ¡æœªè¿è¡Œ / Issue 7: Ollama Service Not Running

**ç—‡çŠ¶ / Symptoms:**
- `Connection refused`
- `Service unavailable`
- `No connection could be made`

**è§£å†³æ–¹æ¡ˆ / Solutions:**

```bash
# 1. æ£€æŸ¥æœåŠ¡çŠ¶æ€ / Check service status
ps aux | grep ollama
pgrep ollama

# 2. å¯åŠ¨OllamaæœåŠ¡ / Start Ollama service
# å‰å°è¿è¡Œ / Run in foreground
ollama serve

# åå°è¿è¡Œ / Run in background
nohup ollama serve > ollama.log 2>&1 &

# 3. éªŒè¯æœåŠ¡å¯åŠ¨ / Verify service startup
curl http://localhost:11434/api/version

# 4. æ£€æŸ¥ç«¯å£å ç”¨ / Check port usage
lsof -i :11434
netstat -tlnp | grep 11434

# 5. é‡å¯æœåŠ¡ / Restart service
pkill ollama
ollama serve &
```

### é—®é¢˜8ï¼šé˜²ç«å¢™é˜»æ­¢è¿æ¥ / Issue 8: Firewall Blocking Connection

**ç—‡çŠ¶ / Symptoms:**
- è¿æ¥è¶…æ—¶
- æ— æ³•è®¿é—®æœ¬åœ°ç«¯å£

**è§£å†³æ–¹æ¡ˆ / Solutions:**

```bash
# macOS
# 1. æ£€æŸ¥é˜²ç«å¢™çŠ¶æ€ / Check firewall status
sudo pfctl -s info

# 2. å…è®¸Ollamaé€šè¿‡é˜²ç«å¢™ / Allow Ollama through firewall
sudo pfctl -f /etc/pf.conf

# Linux (Ubuntu/Debian)
# 1. æ£€æŸ¥é˜²ç«å¢™çŠ¶æ€ / Check firewall status
sudo ufw status

# 2. å…è®¸ç«¯å£è®¿é—® / Allow port access
sudo ufw allow 11434
sudo ufw reload

# 3. æ£€æŸ¥iptablesè§„åˆ™ / Check iptables rules
sudo iptables -L

# Windows
# 1. æ£€æŸ¥Windowsé˜²ç«å¢™è®¾ç½® / Check Windows firewall settings
# 2. æ·»åŠ Ollamaåˆ°é˜²ç«å¢™ä¾‹å¤– / Add Ollama to firewall exceptions
```

## æ€§èƒ½é—®é¢˜ / Performance Issues

### é—®é¢˜9ï¼šå“åº”é€Ÿåº¦æ…¢ / Issue 9: Slow Response Speed

**ç—‡çŠ¶ / Symptoms:**
- ç­‰å¾…æ—¶é—´è¿‡é•¿
- ç³»ç»Ÿèµ„æºå ç”¨é«˜

**è§£å†³æ–¹æ¡ˆ / Solutions:**

```python
# 1. é€‰æ‹©åˆé€‚çš„æ¨¡å‹ / Choose appropriate model
from helper_functions import set_default_model, get_available_models

def optimize_model_selection():
    """ä¼˜åŒ–æ¨¡å‹é€‰æ‹© / Optimize model selection"""
    import psutil
    
    # è·å–ç³»ç»Ÿèµ„æº / Get system resources
    memory_gb = psutil.virtual_memory().total / (1024**3)
    cpu_count = psutil.cpu_count()
    
    print(f"ç³»ç»Ÿå†…å­˜ / System memory: {memory_gb:.1f} GB")
    print(f"CPUæ ¸å¿ƒæ•° / CPU cores: {cpu_count}")
    
    # æ ¹æ®ç³»ç»Ÿèµ„æºæ¨èæ¨¡å‹ / Recommend model based on system resources
    if memory_gb >= 16 and cpu_count >= 8:
        recommended_model = "llama3.1:8b"
        print("æ¨èé«˜æ€§èƒ½æ¨¡å‹ / Recommended high-performance model")
    elif memory_gb >= 8 and cpu_count >= 4:
        recommended_model = "gemma3n:latest"
        print("æ¨èå¹³è¡¡æ¨¡å‹ / Recommended balanced model")
    else:
        recommended_model = "qwen3:0.6b"
        print("æ¨èè½»é‡çº§æ¨¡å‹ / Recommended lightweight model")
    
    set_default_model(recommended_model)
    return recommended_model

# è¿è¡Œä¼˜åŒ– / Run optimization
optimize_model_selection()
```

```bash
# 2. ç³»ç»Ÿä¼˜åŒ– / System optimization
# å…³é—­ä¸å¿…è¦çš„ç¨‹åº / Close unnecessary programs
# å¢åŠ è™šæ‹Ÿå†…å­˜ / Increase virtual memory
# ä½¿ç”¨SSDå­˜å‚¨ / Use SSD storage

# 3. ç›‘æ§èµ„æºä½¿ç”¨ / Monitor resource usage
top -p $(pgrep ollama)
htop
```

### é—®é¢˜10ï¼šå†…å­˜ä¸è¶³ / Issue 10: Out of Memory

**ç—‡çŠ¶ / Symptoms:**
- `Out of memory`
- ç³»ç»Ÿå¡æ­»
- è¿›ç¨‹è¢«æ€æ­»

**è§£å†³æ–¹æ¡ˆ / Solutions:**

```bash
# 1. æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ / Check memory usage
free -h
ps aux --sort=-%mem | head

# 2. é‡Šæ”¾å†…å­˜ / Free memory
sudo sh -c 'echo 3 > /proc/sys/vm/drop_caches'

# 3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ / Use smaller model
ollama pull qwen3:0.6b
export OLLAMA_MODEL="qwen3:0.6b"

# 4. é™åˆ¶å†…å­˜ä½¿ç”¨ / Limit memory usage
export OLLAMA_MAX_MEMORY=4GB
export OLLAMA_MEMORY_LIMIT=4096

# 5. å¢åŠ äº¤æ¢æ–‡ä»¶ / Add swap file
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
```

## å¯¼å…¥é”™è¯¯ / Import Errors

### é—®é¢˜11ï¼šæ¨¡å—å¯¼å…¥å¤±è´¥ / Issue 11: Module Import Failed

**ç—‡çŠ¶ / Symptoms:**
- `ModuleNotFoundError: No module named 'helper_functions'`
- `ImportError: cannot import name 'xxx'`

**è§£å†³æ–¹æ¡ˆ / Solutions:**

```python
# 1. æ£€æŸ¥Pythonè·¯å¾„ / Check Python path
import sys
print("Pythonè·¯å¾„ / Python paths:")
for path in sys.path:
    print(f"  {path}")

# 2. æ‰‹åŠ¨æ·»åŠ é¡¹ç›®è·¯å¾„ / Manually add project path
import os
import sys

# è·å–é¡¹ç›®æ ¹ç›®å½• / Get project root directory
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# 3. ä½¿ç”¨ç›¸å¯¹å¯¼å…¥ / Use relative imports
try:
    from helper_functions import print_llm_response
    print("âœ… æˆåŠŸå¯¼å…¥ / Successfully imported")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥ / Import failed: {e}")
    
    # å°è¯•æ›¿ä»£å¯¼å…¥æ–¹æ³• / Try alternative import methods
    try:
        sys.path.append(os.path.join(os.getcwd(), 'helper_functions'))
        from model_config import get_default_model
        from llm_utils import print_llm_response
        print("âœ… ä½¿ç”¨æ›¿ä»£æ–¹æ³•æˆåŠŸå¯¼å…¥ / Successfully imported using alternative method")
    except ImportError as e2:
        print(f"âŒ æ›¿ä»£æ–¹æ³•ä¹Ÿå¤±è´¥ / Alternative method also failed: {e2}")
```

```bash
# 4. æ£€æŸ¥æ–‡ä»¶ç»“æ„ / Check file structure
find . -name "*.py" -type f | head -20
ls -la helper_functions/

# 5. é‡æ–°å®‰è£…åŒ… / Reinstall package
pip uninstall helper_functions
pip install -e .  # å¦‚æœæœ‰setup.py / If setup.py exists
```

## ç³»ç»Ÿå…¼å®¹æ€§ / System Compatibility

### é—®é¢˜12ï¼šPythonç‰ˆæœ¬å…¼å®¹æ€§ / Issue 12: Python Version Compatibility

**ç—‡çŠ¶ / Symptoms:**
- è¯­æ³•é”™è¯¯
- åŠŸèƒ½ä¸å¯ç”¨
- ç±»å‹æç¤ºé”™è¯¯

**è§£å†³æ–¹æ¡ˆ / Solutions:**

```bash
# 1. æ£€æŸ¥Pythonç‰ˆæœ¬ / Check Python version
python --version
python3 --version

# 2. å‡çº§Python / Upgrade Python
# macOS
brew install python@3.11
brew link python@3.11

# Ubuntu/Debian
sudo apt update
sudo apt install python3.11

# 3. ä½¿ç”¨pyenvç®¡ç†Pythonç‰ˆæœ¬ / Use pyenv to manage Python versions
curl https://pyenv.run | bash
pyenv install 3.11.0
pyenv global 3.11.0

# 4. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ / Create virtual environment
python3.11 -m venv ai-python-env
source ai-python-env/bin/activate
pip install --upgrade pip
```

### é—®é¢˜13ï¼šæ“ä½œç³»ç»Ÿå…¼å®¹æ€§ / Issue 13: Operating System Compatibility

**ç—‡çŠ¶ / Symptoms:**
- å‘½ä»¤ä¸å­˜åœ¨
- è·¯å¾„åˆ†éš”ç¬¦é”™è¯¯
- æƒé™é—®é¢˜

**è§£å†³æ–¹æ¡ˆ / Solutions:**

```python
# 1. è·¨å¹³å°è·¯å¾„å¤„ç† / Cross-platform path handling
import os
from pathlib import Path

def get_safe_path(*paths):
    """è·å–è·¨å¹³å°å®‰å…¨è·¯å¾„ / Get cross-platform safe path"""
    return str(Path(*paths))

# ä½¿ç”¨ç¤ºä¾‹ / Usage example
config_path = get_safe_path("helper_functions", "config.json")
print(f"é…ç½®æ–‡ä»¶è·¯å¾„ / Config file path: {config_path}")

# 2. è·¨å¹³å°å‘½ä»¤æ‰§è¡Œ / Cross-platform command execution
import subprocess
import sys

def run_command(cmd):
    """è·¨å¹³å°å‘½ä»¤æ‰§è¡Œ / Cross-platform command execution"""
    if sys.platform == "win32":
        cmd = ["cmd", "/c"] + cmd if isinstance(cmd, list) else f"cmd /c {cmd}"
    
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return result.stdout, result.stderr, result.returncode
    except Exception as e:
        return "", str(e), 1

# 3. æ£€æŸ¥ç³»ç»Ÿç±»å‹ / Check system type
def get_system_info():
    """è·å–ç³»ç»Ÿä¿¡æ¯ / Get system information"""
    import platform
    
    return {
        'system': platform.system(),
        'release': platform.release(),
        'version': platform.version(),
        'machine': platform.machine(),
        'processor': platform.processor()
    }

print("ç³»ç»Ÿä¿¡æ¯ / System info:", get_system_info())
```

## é«˜çº§æ•…éšœæ’é™¤ / Advanced Troubleshooting

### é—®é¢˜14ï¼šæ·±åº¦è°ƒè¯• / Issue 14: Deep Debugging

å½“å¸¸è§„æ–¹æ³•æ— æ³•è§£å†³é—®é¢˜æ—¶ï¼Œä½¿ç”¨ä»¥ä¸‹é«˜çº§è°ƒè¯•æŠ€å·§ï¼š

When conventional methods fail to solve the problem, use these advanced debugging techniques:

```python
import logging
import traceback
import sys
from datetime import datetime

# 1. å¯ç”¨è¯¦ç»†æ—¥å¿—è®°å½• / Enable verbose logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('debug.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

# 2. åˆ›å»ºè°ƒè¯•è£…é¥°å™¨ / Create debug decorator
def debug_function(func):
    """è°ƒè¯•å‡½æ•°è£…é¥°å™¨ / Debug function decorator"""
    def wrapper(*args, **kwargs):
        logging.debug(f"è°ƒç”¨å‡½æ•° / Calling function: {func.__name__}")
        logging.debug(f"å‚æ•° / Arguments: args={args}, kwargs={kwargs}")
        
        try:
            result = func(*args, **kwargs)
            logging.debug(f"è¿”å›ç»“æœ / Return result: {result}")
            return result
        except Exception as e:
            logging.error(f"å‡½æ•°å¼‚å¸¸ / Function exception: {e}")
            logging.error(f"å †æ ˆè·Ÿè¸ª / Stack trace: {traceback.format_exc()}")
            raise
    
    return wrapper

# 3. ä½¿ç”¨è°ƒè¯•è£…é¥°å™¨ / Use debug decorator
@debug_function
def test_llm_call():
    from helper_functions import print_llm_response
    print_llm_response("This is a test message")

# 4. åˆ›å»ºå®Œæ•´çš„ç³»ç»ŸæŠ¥å‘Š / Create complete system report
def generate_system_report():
    """ç”Ÿæˆå®Œæ•´ç³»ç»ŸæŠ¥å‘Š / Generate complete system report"""
    report = {
        'timestamp': datetime.now().isoformat(),
        'system_info': get_system_info(),
        'python_info': {
            'version': sys.version,
            'executable': sys.executable,
            'path': sys.path
        },
        'environment_variables': dict(os.environ),
        'ollama_status': None,
        'models_available': [],
        'disk_space': None,
        'memory_info': None
    }
    
    # æ·»åŠ æ›´å¤šç³»ç»Ÿä¿¡æ¯...
    try:
        import psutil
        report['memory_info'] = {
            'total': psutil.virtual_memory().total,
            'available': psutil.virtual_memory().available,
            'percent': psutil.virtual_memory().percent
        }
        report['disk_space'] = {
            'total': psutil.disk_usage('/').total,
            'free': psutil.disk_usage('/').free,
            'percent': psutil.disk_usage('/').percent
        }
    except ImportError:
        pass
    
    # ä¿å­˜æŠ¥å‘Š / Save report
    with open('system_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    return report

# è¿è¡Œç³»ç»ŸæŠ¥å‘Š / Run system report
report = generate_system_report()
print("ç³»ç»ŸæŠ¥å‘Šå·²ä¿å­˜åˆ° system_report.json / System report saved to system_report.json")
```

### é—®é¢˜15ï¼šæ€§èƒ½åˆ†æ / Issue 15: Performance Analysis

```python
import time
import cProfile
import pstats
from functools import wraps

def profile_function(func):
    """æ€§èƒ½åˆ†æè£…é¥°å™¨ / Performance profiling decorator"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        pr = cProfile.Profile()
        pr.enable()
        
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        pr.disable()
        
        # åˆ†æç»“æœ / Analyze results
        stats = pstats.Stats(pr)
        stats.sort_stats('cumulative')
        
        print(f"å‡½æ•° {func.__name__} æ‰§è¡Œæ—¶é—´: {end_time - start_time:.4f} ç§’")
        print("æ€§èƒ½åˆ†æç»“æœ:")
        stats.print_stats(10)  # æ˜¾ç¤ºå‰10ä¸ªæœ€è€—æ—¶çš„å‡½æ•°
        
        return result
    
    return wrapper

# ä½¿ç”¨æ€§èƒ½åˆ†æ / Use performance profiling
@profile_function
def test_performance():
    from helper_functions import get_llm_response
    response = get_llm_response("Test performance analysis")
    return response

# è¿è¡Œæ€§èƒ½æµ‹è¯• / Run performance test
test_performance()
```

## è·å–å¸®åŠ© / Getting Help

### ç¤¾åŒºæ”¯æŒ / Community Support

å¦‚æœæœ¬æŒ‡å—æ— æ³•è§£å†³æ‚¨çš„é—®é¢˜ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ¸ é“è·å–å¸®åŠ©ï¼š

If this guide doesn't solve your problem, please get help through these channels:

1. **GitHub Issues**: åœ¨é¡¹ç›®ä»“åº“ä¸­æäº¤è¯¦ç»†çš„é—®é¢˜æŠ¥å‘Š
   **GitHub Issues**: Submit detailed issue reports in the project repository

2. **è®¨è®ºåŒº**: å‚ä¸é¡¹ç›®è®¨è®ºåŒºçš„äº¤æµ
   **Discussions**: Participate in project discussions

3. **é‚®ä»¶æ”¯æŒ**: å‘é€é‚®ä»¶è‡³é¡¹ç›®ç»´æŠ¤è€…
   **Email Support**: Send email to project maintainers

### æäº¤é—®é¢˜æŠ¥å‘Š / Submitting Issue Reports

æäº¤é—®é¢˜æ—¶ï¼Œè¯·åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š

When submitting issues, please include the following information:

```markdown
## é—®é¢˜æè¿° / Problem Description
[è¯¦ç»†æè¿°é—®é¢˜ç°è±¡ / Detailed description of the problem]

## ç¯å¢ƒä¿¡æ¯ / Environment Information
- æ“ä½œç³»ç»Ÿ / Operating System: 
- Pythonç‰ˆæœ¬ / Python Version: 
- Ollamaç‰ˆæœ¬ / Ollama Version: 
- é¡¹ç›®ç‰ˆæœ¬ / Project Version: 

## é‡ç°æ­¥éª¤ / Reproduction Steps
1. [ç¬¬ä¸€æ­¥ / Step 1]
2. [ç¬¬äºŒæ­¥ / Step 2]
3. [ç¬¬ä¸‰æ­¥ / Step 3]

## é”™è¯¯ä¿¡æ¯ / Error Messages
```
[ç²˜è´´å®Œæ•´çš„é”™è¯¯ä¿¡æ¯ / Paste complete error messages]
```

## ç³»ç»ŸæŠ¥å‘Š / System Report
[é™„ä¸Šsystem_report.jsonå†…å®¹ / Attach system_report.json content]

## å·²å°è¯•çš„è§£å†³æ–¹æ¡ˆ / Attempted Solutions
[åˆ—å‡ºå·²ç»å°è¯•çš„è§£å†³æ–¹æ³• / List attempted solutions]
```

---

**æ–‡æ¡£ç‰ˆæœ¬ / Document Version**: v1.0.0  
**æœ€åæ›´æ–° / Last Updated**: 2025-01-18  
**ç»´æŠ¤è€… / Maintainer**: AI Python Bilingual Course Team

å¸Œæœ›è¿™ä¸ªæ•…éšœæ’é™¤æŒ‡å—èƒ½å¸®åŠ©æ‚¨è§£å†³é—®é¢˜ï¼å¦‚æœæ‚¨å‘ç°æ–°çš„é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ä»¬ã€‚

Hope this troubleshooting guide helps you solve your problems! If you find new issues or have improvement suggestions, please feel free to contact us.